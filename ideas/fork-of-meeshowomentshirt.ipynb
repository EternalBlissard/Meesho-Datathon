{"cells":[{"cell_type":"code","execution_count":48,"metadata":{"execution":{"iopub.execute_input":"2024-11-06T16:31:57.231048Z","iopub.status.busy":"2024-11-06T16:31:57.230695Z","iopub.status.idle":"2024-11-06T16:31:57.643691Z","shell.execute_reply":"2024-11-06T16:31:57.642270Z","shell.execute_reply.started":"2024-11-06T16:31:57.230989Z"},"trusted":true},"outputs":[],"source":["import os\n","os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n","import torch\n","import pandas as pd\n","from PIL import Image\n","import numpy as np\n","from torch.utils.data import Dataset\n","import torch\n","import torch.nn as nn\n","# Ignore warnings\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","import random\n","from transformers import (\n","    ViTImageProcessor,\n","    ViTModel,\n","    ViTConfig,\n","    ViTPreTrainedModel,\n","    Trainer, \n","    TrainingArguments,\n","    )\n","import torchvision\n","from sklearn.model_selection import train_test_split\n","import sys\n","from typing import List\n","from sklearn.metrics import classification_report\n","import gc\n","import argparse\n","import wandb\n","import transformers\n","from sklearn.metrics import classification_report,f1_score\n","import torch.nn.init as init\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["epochs=10"]},{"cell_type":"code","execution_count":50,"metadata":{"execution":{"iopub.execute_input":"2024-11-06T16:31:58.441379Z","iopub.status.busy":"2024-11-06T16:31:58.441073Z","iopub.status.idle":"2024-11-06T16:31:58.823798Z","shell.execute_reply":"2024-11-06T16:31:58.823060Z","shell.execute_reply.started":"2024-11-06T16:31:58.441347Z"},"trusted":true},"outputs":[],"source":["CFG = dict(\n","    epochs = epochs,\n","    category = 'women-tshirt',\n","    dropout = True,\n","    dropout_prob = 0.2,\n","    dropna = True,\n","    bestmodel = True\n",")"]},{"cell_type":"code","execution_count":52,"metadata":{"execution":{"iopub.execute_input":"2024-11-06T16:32:03.083318Z","iopub.status.busy":"2024-11-06T16:32:03.082529Z","iopub.status.idle":"2024-11-06T16:32:03.090557Z","shell.execute_reply":"2024-11-06T16:32:03.089594Z","shell.execute_reply.started":"2024-11-06T16:32:03.083272Z"},"trusted":true},"outputs":[],"source":["\n","model_name = 'google/vit-base-patch16-224'\n","save_dir=\"./vit3/\"\n","DEVICE=\"cuda:0\"\n","def setAllSeeds(seed):\n","  os.environ['MY_GLOBAL_SEED'] = str(seed)\n","  random.seed(seed)\n","  np.random.seed(seed)\n","  torch.manual_seed(seed)\n","  torch.cuda.manual_seed_all(seed)\n","# setAllSeeds(42)\n","\n"]},{"cell_type":"code","execution_count":53,"metadata":{"execution":{"iopub.execute_input":"2024-11-06T16:32:03.094412Z","iopub.status.busy":"2024-11-06T16:32:03.094098Z","iopub.status.idle":"2024-11-06T16:32:03.276197Z","shell.execute_reply":"2024-11-06T16:32:03.275382Z","shell.execute_reply.started":"2024-11-06T16:32:03.094380Z"},"trusted":true},"outputs":[],"source":["\n","df = pd.read_csv(\"train.csv\")\n","categories=df[\"Category\"].unique()\n","categories_idx= 1\n","category=categories[categories_idx]\n","df = df[df[\"Category\"]==category]\n","save_dir+=category\n"]},{"cell_type":"code","execution_count":54,"metadata":{"execution":{"iopub.execute_input":"2024-11-06T16:32:03.277709Z","iopub.status.busy":"2024-11-06T16:32:03.277421Z","iopub.status.idle":"2024-11-06T16:32:03.341417Z","shell.execute_reply":"2024-11-06T16:32:03.340561Z","shell.execute_reply.started":"2024-11-06T16:32:03.277679Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["{0: {0: 'multicolor', 1: 'yellow', 2: 'black', 3: 'default', 4: 'pink', 5: 'maroon', 6: 'white'}, 1: {0: 'loose', 1: 'boxy', 2: 'regular'}, 2: {0: 'long', 1: 'crop', 2: 'regular'}, 3: {0: 'default', 1: 'solid', 2: 'printed'}, 4: {0: 'default', 1: 'quirky', 2: 'solid', 3: 'graphic', 4: 'funky print', 5: 'typography'}, 5: {0: 'default', 1: 'long sleeves', 2: 'short sleeves'}, 6: {0: 'regular sleeves', 1: 'cuffed sleeves'}, 7: {0: 'default', 1: 'applique'}}\n","{0: {'multicolor': 0, 'yellow': 1, 'black': 2, 'default': 3, 'pink': 4, 'maroon': 5, 'white': 6}, 1: {'loose': 0, 'boxy': 1, 'regular': 2}, 2: {'long': 0, 'crop': 1, 'regular': 2}, 3: {'default': 0, 'solid': 1, 'printed': 2}, 4: {'default': 0, 'quirky': 1, 'solid': 2, 'graphic': 3, 'funky print': 4, 'typography': 5}, 5: {'default': 0, 'long sleeves': 1, 'short sleeves': 2}, 6: {'regular sleeves': 0, 'cuffed sleeves': 1}, 7: {'default': 0, 'applique': 1}}\n","{0: 'attr_1', 1: 'attr_2', 2: 'attr_3', 3: 'attr_4', 4: 'attr_5', 5: 'attr_6', 6: 'attr_7', 7: 'attr_8'}\n"]}],"source":["\n","delCol = []\n","trackNum = []\n","for i in range(1,11):\n","    uniName = df[\"attr_\"+str(i)].dropna().unique()\n","    if(len(uniName)==0):\n","        delCol.append(\"attr_\"+str(i))\n","    else:\n","        trackNum.append(len(uniName))\n","df = df.drop(delCol,axis=1)\n","\n","\n","id2label={}\n","label2id={}\n","attrs={}\n","total_attr=len(df.columns)\n","for i in range(3,total_attr):\n","    labels=df[df.columns[i]].dropna().unique()\n","    id2label[i-3]={k:labels[k] for k in range(len(labels))}\n","    label2id[i-3]={labels[k]:k for k in range(len(labels))}\n","    attrs[i-3]=df.columns[i]\n","print(id2label)\n","print(label2id)\n","print(attrs)\n"]},{"cell_type":"code","execution_count":55,"metadata":{"execution":{"iopub.execute_input":"2024-11-06T16:32:03.343240Z","iopub.status.busy":"2024-11-06T16:32:03.342776Z","iopub.status.idle":"2024-11-06T16:32:09.279885Z","shell.execute_reply":"2024-11-06T16:32:09.278836Z","shell.execute_reply.started":"2024-11-06T16:32:03.343190Z"},"trusted":true},"outputs":[],"source":["\n","def categorize(example):\n","    for i in attrs:\n","        # print(example[attrs[i]],type(example[attrs[i]]),pd.isna(example[attrs[i]]))\n","        if not pd.isna(example[attrs[i]]):\n","            example[attrs[i]]=label2id[i][example[attrs[i]]]\n","        else:\n","            example[attrs[i]]=-100\n","    return example\n","\n","df=df.apply(categorize,axis=1)\n","effNetWeights = torchvision.models.EfficientNet_B2_Weights.DEFAULT\n","# effNetTransforms = effNetWeights.transforms()\n","processor = effNetWeights.transforms()\n","processor2 = ViTImageProcessor.from_pretrained(model_name)\n","\n","#train test split\n","train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n","\n","\n"]},{"cell_type":"code","execution_count":56,"metadata":{"execution":{"iopub.execute_input":"2024-11-06T16:32:09.281600Z","iopub.status.busy":"2024-11-06T16:32:09.281187Z","iopub.status.idle":"2024-11-06T16:32:09.291646Z","shell.execute_reply":"2024-11-06T16:32:09.290699Z","shell.execute_reply.started":"2024-11-06T16:32:09.281551Z"},"trusted":true},"outputs":[],"source":["\n","class CustomFashionManager(Dataset):\n","    def __init__(self,csv_file, root_dir=\"./\",transforms =None):\n","        self.fashionItems = csv_file\n","        self.root_dir = root_dir\n","        self.transforms = transforms\n","    \n","    def __len__(self):\n","        return len(self.fashionItems)\n","\n","    def __getitem__(self, idx):\n","        if torch.is_tensor(idx):\n","            idx = idx.tolist()\n","\n","        img_name = os.path.join(self.root_dir,f\"{self.fashionItems.iloc[idx, 0]:06d}\"+'.jpg')\n","        image = Image.open(img_name)\n","        attributes = self.fashionItems.iloc[idx, 3:]\n","        attributes = np.array(attributes)\n","        attributes = attributes.astype('float')\n","        inp_image=processor(image)\n","        inp_image = inp_image.reshape(1,inp_image.shape[0],inp_image.shape[1],inp_image.shape[2])\n","#         print(inp_image.shape)\n","        inputs = {'pixel_values':inp_image}\n","#         print(processor2(image,return_tensors='pt')['pixel_values'].shape)\n","        # if self.transforms:\n","        #     inputs = self.transforms(inputs)\n","        inputs['labels']=torch.tensor(attributes, dtype=torch.long)\n","        return inputs\n","\n","train_fashion_data = CustomFashionManager(csv_file=train_df,root_dir='/kaggle/input/visualtaxonomy/train_images')\n","val_fashion_data = CustomFashionManager(csv_file=val_df,root_dir='/kaggle/input/visualtaxonomy/train_images')"]},{"cell_type":"code","execution_count":57,"metadata":{"execution":{"iopub.execute_input":"2024-11-06T16:32:09.293633Z","iopub.status.busy":"2024-11-06T16:32:09.293336Z","iopub.status.idle":"2024-11-06T16:32:09.320575Z","shell.execute_reply":"2024-11-06T16:32:09.319883Z","shell.execute_reply.started":"2024-11-06T16:32:09.293601Z"},"trusted":true},"outputs":[],"source":["\n","class CustomConfig(ViTConfig):\n","    def __init__(self,num_classes_per_label:List[int]=[1],**kwargs):\n","        super().__init__(**kwargs)\n","        self.num_classes_per_label = num_classes_per_label\n","\n","class MultiLabelMultiClassEff(torch.nn.Module):\n","#     config_class=CustomConfig\n","    def __init__(self,num_classes_per_label) -> None:\n","        super().__init__()\n","\n","#         self.vit = ViTModel(config, add_pooling_layer=False)\n","#         self.classifiers = nn.ModuleList([\n","#             nn.Sequential(nn.Dropout(0.2),\n","#             nn.Linear(config.hidden_size, num_classes)) \n","#             for num_classes in config.num_classes_per_label\n","#         ])\n","        effNetWeights = torchvision.models.EfficientNet_B2_Weights.DEFAULT\n","        effNetTransforms = effNetWeights.transforms()\n","        effNet = torchvision.models.efficientnet_b2(weights=effNetWeights)\n","        effNet.classifier = nn.Sequential(\n","#                                     nn.Dropout(p=0.3,inplace=True),\n","                                nn.Linear(1408,768,bias=True))\n","        self.eff = effNet\n","        self.classifiers = nn.ModuleList([\n","            nn.Sequential(nn.Dropout(0.2),\n","            nn.Linear(768, num_classes)) \n","            for num_classes in num_classes_per_label\n","        ])\n","        # Initialize weights and apply final processing\n","#         self.post_init()\n","    \n","    def reinitialize_weights(self):\n","    # Reinitialize ViT layers\n","        for module in self.vit.modules():\n","            if isinstance(module, (nn.Linear, nn.Conv2d)):\n","                init.xavier_uniform_(module.weight)\n","                if module.bias is not None:\n","                    init.zeros_(module.bias)\n","\n","        # Reinitialize classifiers\n","        for classifier in self.classifiers:\n","            init.xavier_uniform_(classifier.weight)\n","            if classifier.bias is not None:\n","                init.zeros_(classifier.bias)\n","\n","    \n","    def forward(self, pixel_values,labels=None):\n","        outputs = self.eff(pixel_values)  # CLS token representation\n","#         print(outputs.shape)\n","        logits = [classifier(outputs) for classifier in self.classifiers]\n","        if labels is not None:\n","            loss=0\n","            for i in range(len(logits)):\n","                target=labels[:,i]\n","                loss += torch.nn.functional.cross_entropy(logits[i], target)\n","            return {\"loss\": loss, \"logits\": logits}\n","        return {\"logits\": logits}\n","\n","class MultiLabelMultiClassViT(torch.nn.Module):\n","    def __init__(self,num_classes_per_label):\n","        super().__init__()\n","        self.model = MultiLabelMultiClassEff(num_classes_per_label)\n","    def save_pretrained(self,output_dir):\n","#         self.model.save\n","        torch.save(self.model.state_dict(), f'{output_dir}/effmodel.pt')\n","    def forward(self, pixel_values,labels=None):\n","        return self.model.forward(pixel_values,labels)\n","    \n","def collate_fn(batch):\n","    return {\n","        'pixel_values': torch.cat([x['pixel_values'] for x in batch],dim=0),\n","        'labels': torch.stack([x['labels'] for x in batch])\n","    }\n","\n","def compute_metrics(pred):\n","    logits = pred.predictions\n","    labels=pred.label_ids\n","    probs = np.stack([np.argmax(logit,axis=1) for logit in logits])\n","    probs=probs.T\n","    labels=labels.flatten()\n","    probs=probs.flatten()\n","    non_padding_indices = [i for i, label in enumerate(labels) if label != -100]\n","    labels = [labels[i] for i in non_padding_indices]\n","    probs = [probs[i] for i in non_padding_indices]\n","    report=classification_report(labels,probs,output_dict=True)\n","    return {'accuracy': report['accuracy'],\"macro avg f1\":report['macro avg']['f1-score']}\n"]},{"cell_type":"code","execution_count":58,"metadata":{"execution":{"iopub.execute_input":"2024-11-06T16:32:09.321848Z","iopub.status.busy":"2024-11-06T16:32:09.321597Z","iopub.status.idle":"2024-11-06T16:32:09.336740Z","shell.execute_reply":"2024-11-06T16:32:09.336009Z","shell.execute_reply.started":"2024-11-06T16:32:09.321820Z"},"trusted":true},"outputs":[],"source":["def compute_metrics3(pred):\n","    logits = pred.predictions\n","    labels=pred.label_ids\n","    probs = np.stack([np.argmax(logit,axis=1) for logit in logits])\n","    probs=probs.T\n","    f1s=[]\n","    for i in range(labels.shape[1]):\n","        non_padding_indices = [j for j, label in enumerate(labels[:,i]) if label != -100]\n","        labels_ = [labels[j,i] for j in non_padding_indices]\n","        probs_ = [probs[j,i] for j in non_padding_indices]\n","        micro=f1_score(labels_,probs_,average='micro')\n","        macro=f1_score(labels_,probs_,average='macro')\n","        print(f\"attr_{i+1} f1 score: {macro}\")\n","        # print(classification_report(labels_,probs_))\n","        score=2*(micro*macro)/(micro+macro)\n","        f1s.append(score)\n","    \n","    wandb.log({'score': sum(f1s)/len(f1s)})\n","    return {'score': sum(f1s)/len(f1s)}\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-11-06T16:32:09.338403Z","iopub.status.busy":"2024-11-06T16:32:09.337990Z"},"trusted":true},"outputs":[{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='146' max='515' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [146/515 05:48 < 14:54, 0.41 it/s, Epoch 1.41/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Score</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>4.193600</td>\n","      <td>2.542357</td>\n","      <td>0.796091</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["attr_1 f1 score: 0.7966287120369325\n","attr_2 f1 score: 0.5215411270916908\n","attr_3 f1 score: 0.7861004002746825\n","attr_4 f1 score: 0.8791000721824377\n","attr_5 f1 score: 0.6873304187083985\n","attr_6 f1 score: 0.8133558660334025\n","attr_7 f1 score: 0.8952917282755462\n","attr_8 f1 score: 0.4885844748858448\n"]}],"source":["\n","# config=ViTConfig.from_pretrained(model_name)\n","# config=CustomConfig(num_classes_per_label=trackNum,**config.to_dict())\n","model = MultiLabelMultiClassEff(trackNum)\n","# model.reinitialize_weights()\n","training_args = TrainingArguments(\n","    output_dir=\"./eff/\"+category+\"basedropout\",\n","    per_device_train_batch_size=128,\n","    per_device_eval_batch_size=128,\n","    evaluation_strategy=\"epoch\",\n","    save_strategy=\"no\",\n","    logging_strategy=\"epoch\",\n","    num_train_epochs=5,\n","    fp16=True,\n","    learning_rate=2e-4,\n","    # save_total_limit=1,\n","    remove_unused_columns=False,\n","    report_to='wandb',\n","#   load_best_model_at_end=True,\n","    metric_for_best_model=\"score\"\n",")\n","\n","trainer = Trainer(\n","    model,\n","    training_args,\n","    train_dataset=train_fashion_data,\n","    eval_dataset=val_fashion_data,\n","    data_collator=collate_fn,\n","    compute_metrics=compute_metrics3,\n","    tokenizer=processor,\n",")\n","trainer.train()\n","#     trainer.save_model(f\"./eff/{category}/finalbasedropout\")\n","#     trainer.evaluate(test_fashion_data)\n","\n","#     del model, trainer\n","#     torch.cuda.empty_cache()\n","#     gc.collect()\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":5987596,"sourceId":9774638,"sourceType":"datasetVersion"}],"dockerImageVersionId":30787,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
