{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08046f22-db0f-45bb-8aa5-18309a23a1e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-14T11:28:23.940808Z",
     "iopub.status.busy": "2024-10-14T11:28:23.940248Z",
     "iopub.status.idle": "2024-10-14T11:28:26.614016Z",
     "shell.execute_reply": "2024-10-14T11:28:26.613452Z",
     "shell.execute_reply.started": "2024-10-14T11:28:23.940788Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "import torch\n",
    "import pandas as pd\n",
    "from PIL import ImageDraw, ImageFont, Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import ViTModel\n",
    "from torchinfo import summary  # \n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import random\n",
    "import time\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from transformers import EfficientNetForImageClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3438730c-888d-4fd7-b57e-8ff63ab4f94a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-14T11:28:26.616389Z",
     "iopub.status.busy": "2024-10-14T11:28:26.616065Z",
     "iopub.status.idle": "2024-10-14T11:28:26.620522Z",
     "shell.execute_reply": "2024-10-14T11:28:26.619968Z",
     "shell.execute_reply.started": "2024-10-14T11:28:26.616371Z"
    }
   },
   "outputs": [],
   "source": [
    "DEVICE=\"cuda:0\"\n",
    "def setAllSeeds(seed):\n",
    "  os.environ['MY_GLOBAL_SEED'] = str(seed)\n",
    "  random.seed(seed)\n",
    "  np.random.seed(seed)\n",
    "  torch.manual_seed(seed)\n",
    "  torch.cuda.manual_seed_all(seed)\n",
    "# setAllSeeds(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78a86c36-a933-42bf-92c1-bd6cfbf0975b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-14T11:28:26.625620Z",
     "iopub.status.busy": "2024-10-14T11:28:26.625171Z",
     "iopub.status.idle": "2024-10-14T11:28:26.717777Z",
     "shell.execute_reply": "2024-10-14T11:28:26.717238Z",
     "shell.execute_reply.started": "2024-10-14T11:28:26.625599Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Men Tshirts' 'Sarees' 'Kurtis' 'Women Tshirts' 'Women Tops & Tunics']\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"train.csv\")\n",
    "categories=df[\"Category\"].unique()\n",
    "print(categories)\n",
    "category=categories[4]\n",
    "df = df[df[\"Category\"]==category]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1c43f05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id              0\n",
       "Category        0\n",
       "len             0\n",
       "attr_1       4946\n",
       "attr_2       5667\n",
       "attr_3       5938\n",
       "attr_4       5553\n",
       "attr_5       6437\n",
       "attr_6       5787\n",
       "attr_7       5788\n",
       "attr_8       5207\n",
       "attr_9       6433\n",
       "attr_10     11823\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae75c4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# attr_i=1\n",
    "# removing_attri=[]\n",
    "# df.iloc[:,3:2+attr_i]=np.nan\n",
    "# df.iloc[:,3+attr_i:]=np.nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8232843b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id     Category             len  attr_1     attr_2   attr_3   attr_4           attr_5  attr_6   attr_7      attr_8         attr_9           attr_10\n",
      "51376  Women Tops & Tunics  10   navy blue  fitted   crop     high             casual  default  solid       short sleeves  default          knitted    1\n",
      "64276  Women Tops & Tunics  10   blue       fitted   crop     sweetheart neck  casual  solid    solid       long sleeves   regular sleeves  default    1\n",
      "64190  Women Tops & Tunics  10   default    boxy     crop     round neck       casual  printed  typography  short sleeves  regular sleeves  tie-ups    1\n",
      "64193  Women Tops & Tunics  10   maroon     regular  regular  round neck       casual  solid    solid       long sleeves   puff sleeves     ruffles    1\n",
      "64199  Women Tops & Tunics  10   black      fitted   regular  round neck       casual  solid    solid       long sleeves   regular sleeves  knitted    1\n",
      "                                                                                                                                                      ..\n",
      "57703  Women Tops & Tunics  10   maroon     regular  regular  round neck       casual  solid    solid       short sleeves  regular sleeves  ruffles    1\n",
      "57707  Women Tops & Tunics  10   red        regular  regular  high             casual  solid    solid       short sleeves  default          default    1\n",
      "57708  Women Tops & Tunics  10   red        boxy     crop     round neck       casual  printed  typography  long sleeves   regular sleeves  tie-ups    1\n",
      "57709  Women Tops & Tunics  10   maroon     fitted   regular  high             casual  solid    solid       short sleeves  regular sleeves  knitted    1\n",
      "70376  Women Tops & Tunics  10   maroon     fitted   crop     round neck       casual  solid    solid       short sleeves  regular sleeves  knitted    1\n",
      "Name: count, Length: 4070, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "218fb75d-1b90-416b-9a41-288de39def1b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-14T11:28:26.719372Z",
     "iopub.status.busy": "2024-10-14T11:28:26.719005Z",
     "iopub.status.idle": "2024-10-14T11:28:26.725099Z",
     "shell.execute_reply": "2024-10-14T11:28:26.724553Z",
     "shell.execute_reply.started": "2024-10-14T11:28:26.719356Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['black' 'navy blue' 'red' 'default' 'maroon' 'white' 'green' 'blue'\n",
      " 'pink' 'yellow' 'peach' 'multicolor']\n",
      "['regular' 'fitted' 'boxy' 'default']\n",
      "['crop' 'regular']\n",
      "['high' 'round neck' 'stylised' 'sweetheart neck' 'v-neck' 'square neck'\n",
      " 'default']\n",
      "['casual' 'party']\n",
      "['default' 'printed' 'solid']\n",
      "['solid' 'typography' 'graphic' 'default' 'quirky' 'floral']\n",
      "['short sleeves' 'sleeveless' 'three-quarter sleeves' 'long sleeves']\n",
      "['regular sleeves' 'default' 'sleeveless' 'puff sleeves']\n",
      "['knitted' 'default' 'ruffles' 'waist tie-ups' 'tie-ups' 'applique']\n"
     ]
    }
   ],
   "source": [
    "delCol = []\n",
    "idxCol = []\n",
    "trackNum = []\n",
    "weights=[]\n",
    "m=nn.Softmax(dim=1)\n",
    "l=[]\n",
    "for i in range(1,11):\n",
    "    uniName = df[\"attr_\"+str(i)].dropna().unique()\n",
    "    print(uniName)\n",
    "    if(len(uniName)==0):\n",
    "        delCol.append(\"attr_\"+str(i))\n",
    "    else:\n",
    "        idxCol.append(\"attr_\"+str(i))\n",
    "        l.append('attr_'+str(i))\n",
    "        trackNum.append(len(uniName))\n",
    "        class_weights=compute_class_weight(class_weight=\"balanced\",classes=uniName,y=df[\"attr_\"+str(i)].dropna())\n",
    "        weights.append(torch.tensor([1]*len(uniName),dtype=torch.float32).to(DEVICE))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52d305da-99e2-424a-b774-b1ffdbd56fcf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-14T11:28:26.732549Z",
     "iopub.status.busy": "2024-10-14T11:28:26.732338Z",
     "iopub.status.idle": "2024-10-14T11:28:26.736656Z",
     "shell.execute_reply": "2024-10-14T11:28:26.736114Z",
     "shell.execute_reply.started": "2024-10-14T11:28:26.732533Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19004, 13)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop(delCol,axis=1)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02ce1ccc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id              0\n",
       "Category        0\n",
       "len             0\n",
       "attr_1       4946\n",
       "attr_2       5667\n",
       "attr_3       5938\n",
       "attr_4       5553\n",
       "attr_5       6437\n",
       "attr_6       5787\n",
       "attr_7       5788\n",
       "attr_8       5207\n",
       "attr_9       6433\n",
       "attr_10     11823\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a251fdd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df2da111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: {0: 'black', 1: 'navy blue', 2: 'red', 3: 'default', 4: 'maroon', 5: 'white', 6: 'green', 7: 'blue', 8: 'pink', 9: 'yellow', 10: 'peach', 11: 'multicolor'}, 1: {0: 'regular', 1: 'fitted', 2: 'boxy', 3: 'default'}, 2: {0: 'crop', 1: 'regular'}, 3: {0: 'high', 1: 'round neck', 2: 'stylised', 3: 'sweetheart neck', 4: 'v-neck', 5: 'square neck', 6: 'default'}, 4: {0: 'casual', 1: 'party'}, 5: {0: 'default', 1: 'printed', 2: 'solid'}, 6: {0: 'solid', 1: 'typography', 2: 'graphic', 3: 'default', 4: 'quirky', 5: 'floral'}, 7: {0: 'short sleeves', 1: 'sleeveless', 2: 'three-quarter sleeves', 3: 'long sleeves'}, 8: {0: 'regular sleeves', 1: 'default', 2: 'sleeveless', 3: 'puff sleeves'}, 9: {0: 'knitted', 1: 'default', 2: 'ruffles', 3: 'waist tie-ups', 4: 'tie-ups', 5: 'applique'}}\n",
      "{0: {'black': 0, 'navy blue': 1, 'red': 2, 'default': 3, 'maroon': 4, 'white': 5, 'green': 6, 'blue': 7, 'pink': 8, 'yellow': 9, 'peach': 10, 'multicolor': 11}, 1: {'regular': 0, 'fitted': 1, 'boxy': 2, 'default': 3}, 2: {'crop': 0, 'regular': 1}, 3: {'high': 0, 'round neck': 1, 'stylised': 2, 'sweetheart neck': 3, 'v-neck': 4, 'square neck': 5, 'default': 6}, 4: {'casual': 0, 'party': 1}, 5: {'default': 0, 'printed': 1, 'solid': 2}, 6: {'solid': 0, 'typography': 1, 'graphic': 2, 'default': 3, 'quirky': 4, 'floral': 5}, 7: {'short sleeves': 0, 'sleeveless': 1, 'three-quarter sleeves': 2, 'long sleeves': 3}, 8: {'regular sleeves': 0, 'default': 1, 'sleeveless': 2, 'puff sleeves': 3}, 9: {'knitted': 0, 'default': 1, 'ruffles': 2, 'waist tie-ups': 3, 'tie-ups': 4, 'applique': 5}}\n",
      "{0: 'attr_1', 1: 'attr_2', 2: 'attr_3', 3: 'attr_4', 4: 'attr_5', 5: 'attr_6', 6: 'attr_7', 7: 'attr_8', 8: 'attr_9', 9: 'attr_10'}\n"
     ]
    }
   ],
   "source": [
    "id2label={}\n",
    "label2id={}\n",
    "attrs={}\n",
    "total_attr=len(df.columns)\n",
    "for i in range(3,total_attr):\n",
    "    labels=df[df.columns[i]].dropna().unique()\n",
    "    # print(df.columns[i],labels)\n",
    "    id2label[i-3]={k:labels[k] for k in range(len(labels))}\n",
    "    label2id[i-3]={labels[k]:k for k in range(len(labels))}\n",
    "    attrs[i-3]=df.columns[i]\n",
    "print(id2label)\n",
    "print(label2id)\n",
    "print(attrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f394a515",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Category</th>\n",
       "      <th>len</th>\n",
       "      <th>attr_1</th>\n",
       "      <th>attr_2</th>\n",
       "      <th>attr_3</th>\n",
       "      <th>attr_4</th>\n",
       "      <th>attr_5</th>\n",
       "      <th>attr_6</th>\n",
       "      <th>attr_7</th>\n",
       "      <th>attr_8</th>\n",
       "      <th>attr_9</th>\n",
       "      <th>attr_10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>51209</th>\n",
       "      <td>51375</td>\n",
       "      <td>Women Tops &amp; Tunics</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>0</td>\n",
       "      <td>-100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51210</th>\n",
       "      <td>51376</td>\n",
       "      <td>Women Tops &amp; Tunics</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51211</th>\n",
       "      <td>51377</td>\n",
       "      <td>Women Tops &amp; Tunics</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51212</th>\n",
       "      <td>51378</td>\n",
       "      <td>Women Tops &amp; Tunics</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51213</th>\n",
       "      <td>51379</td>\n",
       "      <td>Women Tops &amp; Tunics</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id             Category  len  attr_1  attr_2  attr_3  attr_4  \\\n",
       "51209  51375  Women Tops & Tunics   10       0       0    -100    -100   \n",
       "51210  51376  Women Tops & Tunics   10       1       1       0       0   \n",
       "51211  51377  Women Tops & Tunics   10       2       0       1       1   \n",
       "51212  51378  Women Tops & Tunics   10       3       1       0       2   \n",
       "51213  51379  Women Tops & Tunics   10       3       2       1       1   \n",
       "\n",
       "       attr_5  attr_6  attr_7  attr_8  attr_9  attr_10  \n",
       "51209    -100    -100    -100    -100       0     -100  \n",
       "51210       0       0       0       0       1        0  \n",
       "51211       0       1       1       1       2     -100  \n",
       "51212       0       2       0       0       0        1  \n",
       "51213       0       1       1       0       1     -100  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def categorize(example):\n",
    "    for i in attrs:\n",
    "        # print(example[attrs[i]],type(example[attrs[i]]),pd.isna(example[attrs[i]]))\n",
    "        if not pd.isna(example[attrs[i]]):\n",
    "            example[attrs[i]]=label2id[i][example[attrs[i]]]\n",
    "        else:\n",
    "            example[attrs[i]]=-100\n",
    "    return example\n",
    "df=df.apply(categorize,axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f1df7b6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id          0\n",
       "Category    0\n",
       "len         0\n",
       "attr_1      0\n",
       "attr_2      0\n",
       "attr_3      0\n",
       "attr_4      0\n",
       "attr_5      0\n",
       "attr_6      0\n",
       "attr_7      0\n",
       "attr_8      0\n",
       "attr_9      0\n",
       "attr_10     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6778a3b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d886c9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "332990dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "train_df, val_df = train_test_split(df, test_size=0.1, random_state=42)\n",
    "# min_count = train_df[f'attr_{attr_i}'].value_counts().min()\n",
    "# print(f\"The minimum count across all categories is: {min_count}\")\n",
    "# df_sampled = train_df.groupby(f'attr_{attr_i}').sample(n=min_count, random_state=42)\n",
    "# train_df = df_sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "933021b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attr_1\n",
      "-100    4465\n",
      " 0      2429\n",
      " 5      2193\n",
      " 3      1765\n",
      " 8      1366\n",
      " 7       847\n",
      " 9       834\n",
      " 2       723\n",
      " 4       717\n",
      " 6       604\n",
      " 11      409\n",
      " 1       383\n",
      " 10      368\n",
      "Name: count, dtype: int64\n",
      "attr_2\n",
      "-100    5115\n",
      " 1      4949\n",
      " 0      4353\n",
      " 2      1835\n",
      " 3       851\n",
      "Name: count, dtype: int64\n",
      "attr_3\n",
      " 1      6614\n",
      "-100    5346\n",
      " 0      5143\n",
      "Name: count, dtype: int64\n",
      "attr_4\n",
      " 1      6247\n",
      "-100    4993\n",
      " 0      1662\n",
      " 5      1077\n",
      " 4       990\n",
      " 6       981\n",
      " 3       712\n",
      " 2       441\n",
      "Name: count, dtype: int64\n",
      "attr_5\n",
      " 0      11114\n",
      "-100     5799\n",
      " 1        190\n",
      "Name: count, dtype: int64\n",
      "attr_6\n",
      " 2      6999\n",
      "-100    5211\n",
      " 1      4033\n",
      " 0       860\n",
      "Name: count, dtype: int64\n",
      "attr_7\n",
      " 0      7227\n",
      "-100    5216\n",
      " 3      1551\n",
      " 1      1402\n",
      " 4       742\n",
      " 2       520\n",
      " 5       445\n",
      "Name: count, dtype: int64\n",
      "attr_8\n",
      " 0      6679\n",
      "-100    4679\n",
      " 3      3178\n",
      " 2      1635\n",
      " 1       932\n",
      "Name: count, dtype: int64\n",
      "attr_9\n",
      " 0      6724\n",
      "-100    5787\n",
      " 3      2466\n",
      " 1      1471\n",
      " 2       655\n",
      "Name: count, dtype: int64\n",
      "attr_10\n",
      "-100    10666\n",
      " 0       3243\n",
      " 1       1010\n",
      " 2        720\n",
      " 4        553\n",
      " 3        530\n",
      " 5        381\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "for i in l:\n",
    "    print(train_df[i].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e3100d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights[4]=torch.tensor([1,20],dtype=torch.float32).to(DEVICE)\n",
    "# weights[9]=torch.tensor([1,1,1,1,3,2],dtype=torch.float32).to(DEVICE)\n",
    "# weights[7]=torch.tensor([1,10],dtype=torch.float32).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "80e10ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in l:\n",
    "#     print(i)\n",
    "#     for j in set(train_df[i].unique()):\n",
    "#         x=train_df[i][train_df[i]==j]\n",
    "#         if x.count()<300 and j!=-100:\n",
    "#             y=train_df[train_df[i]==j]\n",
    "#             y.iloc[:,3:]=-100\n",
    "#             y[i]=j\n",
    "#             print(y)\n",
    "#             train_df=pd.concat([train_df,y,y]).reset_index(drop=True)\n",
    "# for i in l:\n",
    "#     print(train_df[i].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "36416a7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Category</th>\n",
       "      <th>len</th>\n",
       "      <th>attr_1</th>\n",
       "      <th>attr_2</th>\n",
       "      <th>attr_3</th>\n",
       "      <th>attr_4</th>\n",
       "      <th>attr_5</th>\n",
       "      <th>attr_6</th>\n",
       "      <th>attr_7</th>\n",
       "      <th>attr_8</th>\n",
       "      <th>attr_9</th>\n",
       "      <th>attr_10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>67236</th>\n",
       "      <td>67402</td>\n",
       "      <td>Women Tops &amp; Tunics</td>\n",
       "      <td>10</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>3</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51762</th>\n",
       "      <td>51928</td>\n",
       "      <td>Women Tops &amp; Tunics</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57626</th>\n",
       "      <td>57792</td>\n",
       "      <td>Women Tops &amp; Tunics</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67281</th>\n",
       "      <td>67447</td>\n",
       "      <td>Women Tops &amp; Tunics</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>0</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>2</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68305</th>\n",
       "      <td>68471</td>\n",
       "      <td>Women Tops &amp; Tunics</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-100</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id             Category  len  attr_1  attr_2  attr_3  attr_4  \\\n",
       "67236  67402  Women Tops & Tunics   10    -100    -100    -100    -100   \n",
       "51762  51928  Women Tops & Tunics   10       9       0       1       1   \n",
       "57626  57792  Women Tops & Tunics   10       0       0    -100    -100   \n",
       "67281  67447  Women Tops & Tunics   10       4       0    -100    -100   \n",
       "68305  68471  Women Tops & Tunics   10       7       1       1       1   \n",
       "\n",
       "       attr_5  attr_6  attr_7  attr_8  attr_9  attr_10  \n",
       "67236    -100    -100       3    -100    -100     -100  \n",
       "51762       0       1       5       3       0        1  \n",
       "57626    -100    -100    -100    -100    -100        0  \n",
       "67281       0    -100    -100       2    -100     -100  \n",
       "68305       0       2       0       0    -100        0  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "80a27462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(train_df[f'attr_{attr_i}'].value_counts())\n",
    "# print(val_df[f'attr_{attr_i}'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cfedae55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor\n",
    "# model_name = 'google/vit-base-patch16-224'\n",
    "# model_name=f\"vit5/{category}/final\"\n",
    "# model_name = 'google/efficientnet-b0'\n",
    "# model_name = 'vit6/'+category+'/final'\n",
    "# model_name=\"google/vit-large-patch16-224\"\n",
    "model_name=\"Shadowking912/inception_v3\"\n",
    "# model_name=\"vit6/\"+category+\"/final\"\n",
    "# processor = AutoImageProcessor.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1113befc-d9c0-427e-98c8-467977f0f7ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-14T11:28:26.751798Z",
     "iopub.status.busy": "2024-10-14T11:28:26.751660Z",
     "iopub.status.idle": "2024-10-14T11:28:26.756144Z",
     "shell.execute_reply": "2024-10-14T11:28:26.755669Z",
     "shell.execute_reply.started": "2024-10-14T11:28:26.751778Z"
    }
   },
   "outputs": [],
   "source": [
    "class CustomFashionManager(Dataset):\n",
    "    def __init__(self,csv_file, root_dir=\"./\",transforms=None):\n",
    "        self.fashionItems = csv_file\n",
    "        self.root_dir = root_dir\n",
    "        self.transforms = transforms\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.fashionItems)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_name = os.path.join(self.root_dir,f\"{self.fashionItems.iloc[idx, 0]:06d}\"+'.jpg')\n",
    "        image = Image.open(img_name)\n",
    "        attributes = self.fashionItems.iloc[idx, 3:]\n",
    "        attributes = np.array(attributes)\n",
    "        attributes = attributes.astype('float')\n",
    "        \n",
    "        # inputs=processor(image, return_tensors='pt')\n",
    "        inputs={}\n",
    "        inputs['labels']=torch.tensor(attributes, dtype=torch.long)\n",
    "        inputs['pixel_values'] = self.transforms(image).unsqueeze(0)\n",
    "\n",
    "        return inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "798bdc15-2101-4eb7-a52f-6748fd70d5d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-14T11:28:26.756919Z",
     "iopub.status.busy": "2024-10-14T11:28:26.756780Z",
     "iopub.status.idle": "2024-10-14T11:28:27.051779Z",
     "shell.execute_reply": "2024-10-14T11:28:27.051275Z",
     "shell.execute_reply.started": "2024-10-14T11:28:26.756906Z"
    }
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "transformations=transforms.Compose([\n",
    "    transforms.Resize(299),\n",
    "    transforms.CenterCrop(299),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "# transformations=None\n",
    "train_fashion_data = CustomFashionManager(csv_file=train_df,\n",
    "                                    root_dir='train_images',transforms=transformations)\n",
    "val_fashion_data = CustomFashionManager(csv_file=val_df,\n",
    "                                    root_dir='train_images',transforms=transformations)\n",
    "test_fashion_data = CustomFashionManager(csv_file=df,root_dir='train_images',transforms=transformations)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "06d3d845-6704-4cc0-acdd-a12ee3f1f8c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-14T11:28:27.578464Z",
     "iopub.status.busy": "2024-10-14T11:28:27.578296Z",
     "iopub.status.idle": "2024-10-14T11:28:28.848764Z",
     "shell.execute_reply": "2024-10-14T11:28:28.848059Z",
     "shell.execute_reply.started": "2024-10-14T11:28:27.578419Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from typing import List\n",
    "from transformers import (\n",
    "    ViTConfig,ViTPreTrainedModel,EfficientNetConfig,EfficientNetPreTrainedModel,\n",
    "    EfficientNetModel,PretrainedConfig,PreTrainedModel)\n",
    "from torchvision import models\n",
    "\n",
    "class CustomConfig(PretrainedConfig):\n",
    "    def __init__(self,num_classes_per_label:List[int]=[1],**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_classes_per_label = num_classes_per_label\n",
    "\n",
    "            \n",
    "class MultiLabelMultiClassViT(PreTrainedModel):\n",
    "    config_class=CustomConfig\n",
    "    def __init__(self, config: CustomConfig,pretrain=False) -> None:\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.model = models.inception_v3(pretrained=pretrain)\n",
    "        \n",
    "        self.classifiers = nn.ModuleList([\n",
    "            # nn.Sequential(nn.Dropout(0.2),\n",
    "            # nn.Linear(config.hidden_size, 32),\n",
    "            # nn.ReLU(),\n",
    "            # nn.Dropout(0.2),\n",
    "            nn.Linear(self.model.fc.in_features, num_classes)\n",
    "            for num_classes in config.num_classes_per_label\n",
    "        ])\n",
    "        self.classifiers2 = nn.ModuleList([\n",
    "            # nn.Sequential(nn.Dropout(0.2),\n",
    "            # nn.Linear(config.hidden_size, 32),\n",
    "            # nn.ReLU(),\n",
    "            # nn.Dropout(0.2),\n",
    "            nn.Linear(self.model.AuxLogits.fc.in_features, num_classes)\n",
    "            for num_classes in config.num_classes_per_label\n",
    "        ])\n",
    "        self.model.fc=nn.Identity()\n",
    "        self.model.AuxLogits.fc=nn.Identity()\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    def forward(self, pixel_values,labels=None):\n",
    "        # outputs = self.efficientnet(pixel_values).last_hidden_state[:, 0, :]  # CLS token representation\n",
    "        # outputs = outputs.reshape(outputs.shape[0],-1)\n",
    "        if self.training:\n",
    "            outputs,aux=self.model(pixel_values)\n",
    "            logits = [classifier(outputs) for classifier in self.classifiers]\n",
    "            aux_logits = [classifier(aux) for classifier in self.classifiers2]\n",
    "        else:\n",
    "            outputs=self.model(pixel_values)\n",
    "            logits = [classifier(outputs) for classifier in self.classifiers]\n",
    "        \n",
    "        if labels is not None:\n",
    "            loss=0\n",
    "            # loss_weights=np.array([1,1,1,1,1,1,1,5])\n",
    "            # loss_weights=loss_weights/loss_weights.sum()\n",
    "            for i in range(len(logits)):\n",
    "                target=labels[:,i]\n",
    "                loss += torch.nn.functional.cross_entropy(logits[i], target,ignore_index=-100)\n",
    "                if self.training:\n",
    "                    loss +=0.4*torch.nn.functional.cross_entropy(aux_logits[i], target,ignore_index=-100)\n",
    "            return {\"loss\": loss, \"logits\": logits}\n",
    "        return {\"logits\": logits}\n",
    "\n",
    "    \n",
    "# Example usage\n",
    "num_labels = len(trackNum)  # For example, 5 different labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "871286a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MultiLabelMultiClassViT were not initialized from the model checkpoint at Shadowking912/inception_v3 and are newly initialized: ['classifiers.0.bias', 'classifiers.0.weight', 'classifiers.1.bias', 'classifiers.1.weight', 'classifiers.2.bias', 'classifiers.2.weight', 'classifiers.3.bias', 'classifiers.3.weight', 'classifiers.4.bias', 'classifiers.4.weight', 'classifiers.5.bias', 'classifiers.5.weight', 'classifiers.6.bias', 'classifiers.6.weight', 'classifiers.7.bias', 'classifiers.7.weight', 'classifiers.8.bias', 'classifiers.8.weight', 'classifiers.9.bias', 'classifiers.9.weight', 'classifiers2.0.bias', 'classifiers2.0.weight', 'classifiers2.1.bias', 'classifiers2.1.weight', 'classifiers2.2.bias', 'classifiers2.2.weight', 'classifiers2.3.bias', 'classifiers2.3.weight', 'classifiers2.4.bias', 'classifiers2.4.weight', 'classifiers2.5.bias', 'classifiers2.5.weight', 'classifiers2.6.bias', 'classifiers2.6.weight', 'classifiers2.7.bias', 'classifiers2.7.weight', 'classifiers2.8.bias', 'classifiers2.8.weight', 'classifiers2.9.bias', 'classifiers2.9.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "MultiLabelMultiClassViT                  --\n",
      "├─Inception3: 1-1                        --\n",
      "│    └─BasicConv2d: 2-1                  --\n",
      "│    │    └─Conv2d: 3-1                  864\n",
      "│    │    └─BatchNorm2d: 3-2             64\n",
      "│    └─BasicConv2d: 2-2                  --\n",
      "│    │    └─Conv2d: 3-3                  9,216\n",
      "│    │    └─BatchNorm2d: 3-4             64\n",
      "│    └─BasicConv2d: 2-3                  --\n",
      "│    │    └─Conv2d: 3-5                  18,432\n",
      "│    │    └─BatchNorm2d: 3-6             128\n",
      "│    └─MaxPool2d: 2-4                    --\n",
      "│    └─BasicConv2d: 2-5                  --\n",
      "│    │    └─Conv2d: 3-7                  5,120\n",
      "│    │    └─BatchNorm2d: 3-8             160\n",
      "│    └─BasicConv2d: 2-6                  --\n",
      "│    │    └─Conv2d: 3-9                  138,240\n",
      "│    │    └─BatchNorm2d: 3-10            384\n",
      "│    └─MaxPool2d: 2-7                    --\n",
      "│    └─InceptionA: 2-8                   --\n",
      "│    │    └─BasicConv2d: 3-11            12,416\n",
      "│    │    └─BasicConv2d: 3-12            9,312\n",
      "│    │    └─BasicConv2d: 3-13            76,928\n",
      "│    │    └─BasicConv2d: 3-14            12,416\n",
      "│    │    └─BasicConv2d: 3-15            55,488\n",
      "│    │    └─BasicConv2d: 3-16            83,136\n",
      "│    │    └─BasicConv2d: 3-17            6,208\n",
      "│    └─InceptionA: 2-9                   --\n",
      "│    │    └─BasicConv2d: 3-18            16,512\n",
      "│    │    └─BasicConv2d: 3-19            12,384\n",
      "│    │    └─BasicConv2d: 3-20            76,928\n",
      "│    │    └─BasicConv2d: 3-21            16,512\n",
      "│    │    └─BasicConv2d: 3-22            55,488\n",
      "│    │    └─BasicConv2d: 3-23            83,136\n",
      "│    │    └─BasicConv2d: 3-24            16,512\n",
      "│    └─InceptionA: 2-10                  --\n",
      "│    │    └─BasicConv2d: 3-25            18,560\n",
      "│    │    └─BasicConv2d: 3-26            13,920\n",
      "│    │    └─BasicConv2d: 3-27            76,928\n",
      "│    │    └─BasicConv2d: 3-28            18,560\n",
      "│    │    └─BasicConv2d: 3-29            55,488\n",
      "│    │    └─BasicConv2d: 3-30            83,136\n",
      "│    │    └─BasicConv2d: 3-31            18,560\n",
      "│    └─InceptionB: 2-11                  --\n",
      "│    │    └─BasicConv2d: 3-32            996,096\n",
      "│    │    └─BasicConv2d: 3-33            18,560\n",
      "│    │    └─BasicConv2d: 3-34            55,488\n",
      "│    │    └─BasicConv2d: 3-35            83,136\n",
      "│    └─InceptionC: 2-12                  --\n",
      "│    │    └─BasicConv2d: 3-36            147,840\n",
      "│    │    └─BasicConv2d: 3-37            98,560\n",
      "│    │    └─BasicConv2d: 3-38            114,944\n",
      "│    │    └─BasicConv2d: 3-39            172,416\n",
      "│    │    └─BasicConv2d: 3-40            98,560\n",
      "│    │    └─BasicConv2d: 3-41            114,944\n",
      "│    │    └─BasicConv2d: 3-42            114,944\n",
      "│    │    └─BasicConv2d: 3-43            114,944\n",
      "│    │    └─BasicConv2d: 3-44            172,416\n",
      "│    │    └─BasicConv2d: 3-45            147,840\n",
      "│    └─InceptionC: 2-13                  --\n",
      "│    │    └─BasicConv2d: 3-46            147,840\n",
      "│    │    └─BasicConv2d: 3-47            123,200\n",
      "│    │    └─BasicConv2d: 3-48            179,520\n",
      "│    │    └─BasicConv2d: 3-49            215,424\n",
      "│    │    └─BasicConv2d: 3-50            123,200\n",
      "│    │    └─BasicConv2d: 3-51            179,520\n",
      "│    │    └─BasicConv2d: 3-52            179,520\n",
      "│    │    └─BasicConv2d: 3-53            179,520\n",
      "│    │    └─BasicConv2d: 3-54            215,424\n",
      "│    │    └─BasicConv2d: 3-55            147,840\n",
      "│    └─InceptionC: 2-14                  --\n",
      "│    │    └─BasicConv2d: 3-56            147,840\n",
      "│    │    └─BasicConv2d: 3-57            123,200\n",
      "│    │    └─BasicConv2d: 3-58            179,520\n",
      "│    │    └─BasicConv2d: 3-59            215,424\n",
      "│    │    └─BasicConv2d: 3-60            123,200\n",
      "│    │    └─BasicConv2d: 3-61            179,520\n",
      "│    │    └─BasicConv2d: 3-62            179,520\n",
      "│    │    └─BasicConv2d: 3-63            179,520\n",
      "│    │    └─BasicConv2d: 3-64            215,424\n",
      "│    │    └─BasicConv2d: 3-65            147,840\n",
      "│    └─InceptionC: 2-15                  --\n",
      "│    │    └─BasicConv2d: 3-66            147,840\n",
      "│    │    └─BasicConv2d: 3-67            147,840\n",
      "│    │    └─BasicConv2d: 3-68            258,432\n",
      "│    │    └─BasicConv2d: 3-69            258,432\n",
      "│    │    └─BasicConv2d: 3-70            147,840\n",
      "│    │    └─BasicConv2d: 3-71            258,432\n",
      "│    │    └─BasicConv2d: 3-72            258,432\n",
      "│    │    └─BasicConv2d: 3-73            258,432\n",
      "│    │    └─BasicConv2d: 3-74            258,432\n",
      "│    │    └─BasicConv2d: 3-75            147,840\n",
      "│    └─InceptionAux: 2-16                --\n",
      "│    │    └─BasicConv2d: 3-76            98,560\n",
      "│    │    └─BasicConv2d: 3-77            2,459,136\n",
      "│    │    └─Identity: 3-78               --\n",
      "│    └─InceptionD: 2-17                  --\n",
      "│    │    └─BasicConv2d: 3-79            147,840\n",
      "│    │    └─BasicConv2d: 3-80            553,600\n",
      "│    │    └─BasicConv2d: 3-81            147,840\n",
      "│    │    └─BasicConv2d: 3-82            258,432\n",
      "│    │    └─BasicConv2d: 3-83            258,432\n",
      "│    │    └─BasicConv2d: 3-84            332,160\n",
      "│    └─InceptionE: 2-18                  --\n",
      "│    │    └─BasicConv2d: 3-85            410,240\n",
      "│    │    └─BasicConv2d: 3-86            492,288\n",
      "│    │    └─BasicConv2d: 3-87            443,136\n",
      "│    │    └─BasicConv2d: 3-88            443,136\n",
      "│    │    └─BasicConv2d: 3-89            574,336\n",
      "│    │    └─BasicConv2d: 3-90            1,549,056\n",
      "│    │    └─BasicConv2d: 3-91            443,136\n",
      "│    │    └─BasicConv2d: 3-92            443,136\n",
      "│    │    └─BasicConv2d: 3-93            246,144\n",
      "│    └─InceptionE: 2-19                  --\n",
      "│    │    └─BasicConv2d: 3-94            656,000\n",
      "│    │    └─BasicConv2d: 3-95            787,200\n",
      "│    │    └─BasicConv2d: 3-96            443,136\n",
      "│    │    └─BasicConv2d: 3-97            443,136\n",
      "│    │    └─BasicConv2d: 3-98            918,400\n",
      "│    │    └─BasicConv2d: 3-99            1,549,056\n",
      "│    │    └─BasicConv2d: 3-100           443,136\n",
      "│    │    └─BasicConv2d: 3-101           443,136\n",
      "│    │    └─BasicConv2d: 3-102           393,600\n",
      "│    └─AdaptiveAvgPool2d: 2-20           --\n",
      "│    └─Dropout: 2-21                     --\n",
      "│    └─Identity: 2-22                    --\n",
      "├─ModuleList: 1-2                        --\n",
      "│    └─Linear: 2-23                      24,588\n",
      "│    └─Linear: 2-24                      8,196\n",
      "│    └─Linear: 2-25                      4,098\n",
      "│    └─Linear: 2-26                      14,343\n",
      "│    └─Linear: 2-27                      4,098\n",
      "│    └─Linear: 2-28                      6,147\n",
      "│    └─Linear: 2-29                      12,294\n",
      "│    └─Linear: 2-30                      8,196\n",
      "│    └─Linear: 2-31                      8,196\n",
      "│    └─Linear: 2-32                      12,294\n",
      "├─ModuleList: 1-3                        --\n",
      "│    └─Linear: 2-33                      9,228\n",
      "│    └─Linear: 2-34                      3,076\n",
      "│    └─Linear: 2-35                      1,538\n",
      "│    └─Linear: 2-36                      5,383\n",
      "│    └─Linear: 2-37                      1,538\n",
      "│    └─Linear: 2-38                      2,307\n",
      "│    └─Linear: 2-39                      4,614\n",
      "│    └─Linear: 2-40                      3,076\n",
      "│    └─Linear: 2-41                      3,076\n",
      "│    └─Linear: 2-42                      4,614\n",
      "=================================================================\n",
      "Total params: 24,484,164\n",
      "Trainable params: 24,484,164\n",
      "Non-trainable params: 0\n",
      "=================================================================\n"
     ]
    }
   ],
   "source": [
    "config=CustomConfig(num_classes_per_label=trackNum)\n",
    "model = MultiLabelMultiClassViT.from_pretrained(model_name, config=config,pretrain=True)\n",
    "# # model.model.aux_logits=False\n",
    "\n",
    "# model=MultiLabelMultiClassViT.from_pretrained(model_name)\n",
    "print(summary(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9be5327a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "from sklearn.metrics import classification_report,f1_score\n",
    "batch_size = 32\n",
    "def collate_fn(batch):\n",
    "    return {\n",
    "        'pixel_values': torch.cat([x['pixel_values'] for x in batch],dim=0),\n",
    "        'labels': torch.stack([x['labels'] for x in batch])\n",
    "    }\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    logits = pred.predictions\n",
    "    labels=pred.label_ids\n",
    "    probs = np.stack([np.argmax(logit,axis=1) for logit in logits])\n",
    "    probs=probs.T\n",
    "    f1s=[]\n",
    "    for i in range(labels.shape[1]):\n",
    "        non_padding_indices = [j for j, label in enumerate(labels[:,i]) if label != -100]\n",
    "        labels_ = [labels[j,i] for j in non_padding_indices]\n",
    "        probs_ = [probs[j,i] for j in non_padding_indices]\n",
    "        micro=f1_score(labels_,probs_,average='micro')\n",
    "        macro=f1_score(labels_,probs_,average='macro')\n",
    "        print(f\"attr_{i} f1 score: {macro}\")\n",
    "        # print(classification_report(labels_,probs_))\n",
    "        score=2*(micro*macro)/(micro+macro)\n",
    "        f1s.append(score)\n",
    "    print()\n",
    "    print()\n",
    "    return {'score': sum(f1s)/len(f1s)}\n",
    "\n",
    "\n",
    "    # logits = pred.predictions\n",
    "    # labels=pred.label_ids\n",
    "    # probs = np.stack([np.argmax(logit,axis=1) for logit in logits])\n",
    "    # probs=probs.T\n",
    "    # labels=labels.flatten()\n",
    "    # probs=probs.flatten()\n",
    "    # non_padding_indices = [i for i, label in enumerate(labels) if label != -100]\n",
    "    # labels = [labels[i] for i in non_padding_indices]\n",
    "    # probs = [probs[i] for i in non_padding_indices]\n",
    "    # print(classification_report(labels,probs))\n",
    "    # report=classification_report(labels,probs,output_dict=True)\n",
    "    # return {'accuracy': report['accuracy'],\"macro avg f1\":report['macro avg']['f1-score']}\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "  output_dir=\"./vit6/\"+category,\n",
    "  per_device_train_batch_size=128,\n",
    "  per_device_eval_batch_size=128,\n",
    "  evaluation_strategy=\"epoch\",\n",
    "  save_strategy=\"epoch\",\n",
    "  logging_strategy=\"epoch\",\n",
    "  num_train_epochs=10,\n",
    "  bf16=True,\n",
    "  learning_rate=2e-4,\n",
    "#   dataloader_num_workers=20,lr\n",
    "  save_total_limit=1,\n",
    "  remove_unused_columns=False,\n",
    "  push_to_hub=False,\n",
    "  report_to='wandb',\n",
    "  load_best_model_at_end=True,\n",
    "  metric_for_best_model=\"score\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=train_fashion_data,\n",
    "    eval_dataset=val_fashion_data,\n",
    "    data_collator=collate_fn,\n",
    "    compute_metrics=compute_metrics,\n",
    "    # tokenizer=processor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccb554e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-11-16 23:41:14,468] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aseems/anaconda3/envs/mhcp4/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-dev package with apt\n",
      "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
      "\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n",
      "\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4\n",
      "\u001b[93m [WARNING] \u001b[0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkaran21258\u001b[0m (\u001b[33mkaran912\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.18.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/aseems/ml/mchl/wandb/run-20241116_234117-zxxgdmy0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/karan912/huggingface/runs/zxxgdmy0' target=\"_blank\">./vit6/Women Tops & Tunics</a></strong> to <a href='https://wandb.ai/karan912/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/karan912/huggingface' target=\"_blank\">https://wandb.ai/karan912/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/karan912/huggingface/runs/zxxgdmy0' target=\"_blank\">https://wandb.ai/karan912/huggingface/runs/zxxgdmy0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='490' max='1340' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 490/1340 08:44 < 15:12, 0.93 it/s, Epoch 3.65/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.230480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.230480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.230480</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attr_0 f1 score: 0.02695789059425423\n",
      "attr_1 f1 score: 0.13382193268186754\n",
      "attr_2 f1 score: 0.3011211959423385\n",
      "attr_3 f1 score: 0.0343082114735658\n",
      "attr_4 f1 score: 0.4962106102911847\n",
      "attr_5 f1 score: 0.03840682788051209\n",
      "attr_6 f1 score: 0.1236788136930115\n",
      "attr_7 f1 score: 0.17709313264346188\n",
      "attr_8 f1 score: 0.1879661859771258\n",
      "attr_9 f1 score: 0.11366991437850604\n",
      "\n",
      "\n",
      "attr_0 f1 score: 0.02695789059425423\n",
      "attr_1 f1 score: 0.13382193268186754\n",
      "attr_2 f1 score: 0.3011211959423385\n",
      "attr_3 f1 score: 0.0343082114735658\n",
      "attr_4 f1 score: 0.4962106102911847\n",
      "attr_5 f1 score: 0.03840682788051209\n",
      "attr_6 f1 score: 0.1236788136930115\n",
      "attr_7 f1 score: 0.17709313264346188\n",
      "attr_8 f1 score: 0.1879661859771258\n",
      "attr_9 f1 score: 0.11366991437850604\n",
      "\n",
      "\n",
      "attr_0 f1 score: 0.02695789059425423\n",
      "attr_1 f1 score: 0.13382193268186754\n",
      "attr_2 f1 score: 0.3011211959423385\n",
      "attr_3 f1 score: 0.0343082114735658\n",
      "attr_4 f1 score: 0.4962106102911847\n",
      "attr_5 f1 score: 0.03840682788051209\n",
      "attr_6 f1 score: 0.1236788136930115\n",
      "attr_7 f1 score: 0.17709313264346188\n",
      "attr_8 f1 score: 0.1879661859771258\n",
      "attr_9 f1 score: 0.11366991437850604\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "trainer.save_model(f\"./vit6/{category}/final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3474278",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='72' max='144' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 72/144 01:03 < 01:04, 1.12 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_fashion_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#1-54(imbalance),2-65,3-85,5-56,6-70,7-52(imbalance),8-51(imbalance),9-60(imbalance),10-49(imbalance)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#4-30,\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/trainer.py:3868\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3865\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   3867\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[0;32m-> 3868\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3869\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3870\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvaluation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3871\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[1;32m   3872\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[1;32m   3873\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   3874\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3875\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3876\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3878\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[1;32m   3879\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/trainer.py:4051\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   4048\u001b[0m observed_num_examples \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   4050\u001b[0m \u001b[38;5;66;03m# Main evaluation loop\u001b[39;00m\n\u001b[0;32m-> 4051\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[1;32m   4052\u001b[0m     \u001b[38;5;66;03m# Update the observed num examples\u001b[39;00m\n\u001b[1;32m   4053\u001b[0m     observed_batch_size \u001b[38;5;241m=\u001b[39m find_batch_size(inputs)\n\u001b[1;32m   4054\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m observed_batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/accelerate/data_loader.py:561\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     current_batch \u001b[38;5;241m=\u001b[39m send_to_device(current_batch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice, non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_non_blocking)\n\u001b[1;32m    560\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_state_dict()\n\u001b[0;32m--> 561\u001b[0m next_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskip_batches:\n\u001b[1;32m    563\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m current_batch\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[44], line 23\u001b[0m, in \u001b[0;36mCustomFashionManager.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     21\u001b[0m inputs\u001b[38;5;241m=\u001b[39m{}\n\u001b[1;32m     22\u001b[0m inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mtensor(attributes, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[0;32m---> 23\u001b[0m inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpixel_values\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransforms\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m inputs\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torchvision/transforms/transforms.py:354\u001b[0m, in \u001b[0;36mResize.forward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m    347\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;124;03m        img (PIL Image or Tensor): Image to be scaled.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;124;03m        PIL Image or Tensor: Rescaled image.\u001b[39;00m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 354\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mantialias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torchvision/transforms/functional.py:477\u001b[0m, in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[1;32m    475\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnti-alias option is always applied for PIL Image input. Argument antialias is ignored.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    476\u001b[0m     pil_interpolation \u001b[38;5;241m=\u001b[39m pil_modes_mapping[interpolation]\n\u001b[0;32m--> 477\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_pil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpil_interpolation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F_t\u001b[38;5;241m.\u001b[39mresize(img, size\u001b[38;5;241m=\u001b[39moutput_size, interpolation\u001b[38;5;241m=\u001b[39minterpolation\u001b[38;5;241m.\u001b[39mvalue, antialias\u001b[38;5;241m=\u001b[39mantialias)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torchvision/transforms/_functional_pil.py:250\u001b[0m, in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(size, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(size) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m):\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot inappropriate size arg: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 250\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/PIL/Image.py:2222\u001b[0m, in \u001b[0;36mImage.resize\u001b[0;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[1;32m   2210\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2211\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduce(factor, box\u001b[38;5;241m=\u001b[39mreduce_box)\n\u001b[1;32m   2212\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduce)\n\u001b[1;32m   2213\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m Image\u001b[38;5;241m.\u001b[39mreduce(\u001b[38;5;28mself\u001b[39m, factor, box\u001b[38;5;241m=\u001b[39mreduce_box)\n\u001b[1;32m   2214\u001b[0m         )\n\u001b[1;32m   2215\u001b[0m         box \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2216\u001b[0m             (box[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_x,\n\u001b[1;32m   2217\u001b[0m             (box[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_y,\n\u001b[1;32m   2218\u001b[0m             (box[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_x,\n\u001b[1;32m   2219\u001b[0m             (box[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_y,\n\u001b[1;32m   2220\u001b[0m         )\n\u001b[0;32m-> 2222\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbox\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.evaluate(test_fashion_data)\n",
    "#1-54(imbalance),2-65,3-85,5-56,6-70,7-52(imbalance),8-51(imbalance),9-60(imbalance),10-49(imbalance)\n",
    "#4-30,"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mhcp4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
