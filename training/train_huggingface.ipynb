{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08046f22-db0f-45bb-8aa5-18309a23a1e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-14T11:28:23.940808Z",
     "iopub.status.busy": "2024-10-14T11:28:23.940248Z",
     "iopub.status.idle": "2024-10-14T11:28:26.614016Z",
     "shell.execute_reply": "2024-10-14T11:28:26.613452Z",
     "shell.execute_reply.started": "2024-10-14T11:28:23.940788Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "import torch\n",
    "import pandas as pd\n",
    "from PIL import ImageDraw, ImageFont, Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import ViTModel\n",
    "from torchinfo import summary  # \n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3438730c-888d-4fd7-b57e-8ff63ab4f94a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-14T11:28:26.616389Z",
     "iopub.status.busy": "2024-10-14T11:28:26.616065Z",
     "iopub.status.idle": "2024-10-14T11:28:26.620522Z",
     "shell.execute_reply": "2024-10-14T11:28:26.619968Z",
     "shell.execute_reply.started": "2024-10-14T11:28:26.616371Z"
    }
   },
   "outputs": [],
   "source": [
    "DEVICE=\"cuda:0\"\n",
    "def setAllSeeds(seed):\n",
    "  os.environ['MY_GLOBAL_SEED'] = str(seed)\n",
    "  random.seed(seed)\n",
    "  np.random.seed(seed)\n",
    "  torch.manual_seed(seed)\n",
    "  torch.cuda.manual_seed_all(seed)\n",
    "setAllSeeds(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d40ebce0-ebc0-4d20-b499-76b11ff9f178",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-14T11:28:26.621515Z",
     "iopub.status.busy": "2024-10-14T11:28:26.621315Z",
     "iopub.status.idle": "2024-10-14T11:28:26.624377Z",
     "shell.execute_reply": "2024-10-14T11:28:26.623901Z",
     "shell.execute_reply.started": "2024-10-14T11:28:26.621498Z"
    }
   },
   "outputs": [],
   "source": [
    "def show_fashionImage(image):\n",
    "    \"\"\"Show image with landmarks\"\"\"\n",
    "    plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78a86c36-a933-42bf-92c1-bd6cfbf0975b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-14T11:28:26.625620Z",
     "iopub.status.busy": "2024-10-14T11:28:26.625171Z",
     "iopub.status.idle": "2024-10-14T11:28:26.717777Z",
     "shell.execute_reply": "2024-10-14T11:28:26.717238Z",
     "shell.execute_reply.started": "2024-10-14T11:28:26.625599Z"
    }
   },
   "outputs": [],
   "source": [
    "category=\"Men Tshirts\"\n",
    "df = pd.read_csv(\"train.csv\")\n",
    "df = df[df[\"Category\"]==category]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "218fb75d-1b90-416b-9a41-288de39def1b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-14T11:28:26.719372Z",
     "iopub.status.busy": "2024-10-14T11:28:26.719005Z",
     "iopub.status.idle": "2024-10-14T11:28:26.725099Z",
     "shell.execute_reply": "2024-10-14T11:28:26.724553Z",
     "shell.execute_reply.started": "2024-10-14T11:28:26.719356Z"
    }
   },
   "outputs": [],
   "source": [
    "delCol = []\n",
    "idxCol = []\n",
    "trackNum = []\n",
    "for i in range(1,11):\n",
    "    uniName = df[\"attr_\"+str(i)].unique()\n",
    "    # print(len(uniName))\n",
    "    if(len(uniName)==1):\n",
    "        delCol.append(\"attr_\"+str(i))\n",
    "    else:\n",
    "        idxCol.append(\"attr_\"+str(i))\n",
    "        trackNum.append(len(uniName))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dbbda5fd-b688-4b4c-bbef-7dc6469e3fb3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-14T11:28:26.725738Z",
     "iopub.status.busy": "2024-10-14T11:28:26.725595Z",
     "iopub.status.idle": "2024-10-14T11:28:26.731171Z",
     "shell.execute_reply": "2024-10-14T11:28:26.730154Z",
     "shell.execute_reply.started": "2024-10-14T11:28:26.725724Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['attr_6', 'attr_7', 'attr_8', 'attr_9', 'attr_10']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delCol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52d305da-99e2-424a-b774-b1ffdbd56fcf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-14T11:28:26.732549Z",
     "iopub.status.busy": "2024-10-14T11:28:26.732338Z",
     "iopub.status.idle": "2024-10-14T11:28:26.736656Z",
     "shell.execute_reply": "2024-10-14T11:28:26.736114Z",
     "shell.execute_reply.started": "2024-10-14T11:28:26.732533Z"
    }
   },
   "outputs": [],
   "source": [
    "df = df.drop(delCol,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15214077-0745-494c-b6db-af0d0c0b581c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-14T11:28:26.737487Z",
     "iopub.status.busy": "2024-10-14T11:28:26.737307Z",
     "iopub.status.idle": "2024-10-14T11:28:26.742621Z",
     "shell.execute_reply": "2024-10-14T11:28:26.742124Z",
     "shell.execute_reply.started": "2024-10-14T11:28:26.737472Z"
    }
   },
   "outputs": [],
   "source": [
    "df = df.fillna(\"NA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cbfadcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df=df[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df2da111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: {0: 'default', 1: 'multicolor', 2: 'black', 3: 'white', 4: 'NA'}, 1: {0: 'round', 1: 'polo', 2: 'NA'}, 2: {0: 'printed', 1: 'solid', 2: 'NA'}, 3: {0: 'default', 1: 'solid', 2: 'NA', 3: 'typography'}, 4: {0: 'short sleeves', 1: 'long sleeves', 2: 'NA'}}\n",
      "{0: {'default': 0, 'multicolor': 1, 'black': 2, 'white': 3, 'NA': 4}, 1: {'round': 0, 'polo': 1, 'NA': 2}, 2: {'printed': 0, 'solid': 1, 'NA': 2}, 3: {'default': 0, 'solid': 1, 'NA': 2, 'typography': 3}, 4: {'short sleeves': 0, 'long sleeves': 1, 'NA': 2}}\n",
      "{0: 'attr_1', 1: 'attr_2', 2: 'attr_3', 3: 'attr_4', 4: 'attr_5'}\n"
     ]
    }
   ],
   "source": [
    "id2label={}\n",
    "label2id={}\n",
    "attrs={}\n",
    "total_attr=len(df.columns)\n",
    "for i in range(3,total_attr):\n",
    "    labels=df[df.columns[i]].unique()\n",
    "    # print(df.columns[i],labels)\n",
    "    id2label[i-3]={k:labels[k] for k in range(len(labels))}\n",
    "    label2id[i-3]={labels[k]:k for k in range(len(labels))}\n",
    "    attrs[i-3]=df.columns[i]\n",
    "print(id2label)\n",
    "print(label2id)\n",
    "print(attrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f394a515",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize(example):\n",
    "    for i in attrs:\n",
    "        example[attrs[i]]=label2id[i][example[attrs[i]]]\n",
    "    return example\n",
    "df=df.apply(categorize,axis=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "92d71000",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTImageProcessor\n",
    "model_name = 'google/vit-base-patch16-224'\n",
    "processor = ViTImageProcessor.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "332990dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, val_df = train_test_split(df, test_size=0.3)\n",
    "val_df,test_df=train_test_split(val_df,test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1113befc-d9c0-427e-98c8-467977f0f7ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-14T11:28:26.751798Z",
     "iopub.status.busy": "2024-10-14T11:28:26.751660Z",
     "iopub.status.idle": "2024-10-14T11:28:26.756144Z",
     "shell.execute_reply": "2024-10-14T11:28:26.755669Z",
     "shell.execute_reply.started": "2024-10-14T11:28:26.751778Z"
    }
   },
   "outputs": [],
   "source": [
    "class CustomFashionManager(Dataset):\n",
    "    def __init__(self,csv_file, root_dir=\"./\",transforms =None):\n",
    "        self.fashionItems = csv_file\n",
    "        self.root_dir = root_dir\n",
    "        self.transforms = transforms\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.fashionItems)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_name = os.path.join(self.root_dir,f\"{self.fashionItems.iloc[idx, 0]:06d}\"+'.jpg')\n",
    "        image = Image.open(img_name)\n",
    "        attributes = self.fashionItems.iloc[idx, 3:]\n",
    "        attributes = np.array(attributes)\n",
    "        attributes = attributes.astype('float')\n",
    "        # print(attributes.shape)\n",
    "        # attributes = attributes.astype('float').reshape(-1, len(attributes))\n",
    "        # swap color axis because\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C X H X W\n",
    "        inputs=processor(image, return_tensors='pt')\n",
    "        inputs['labels']=torch.tensor(attributes, dtype=torch.long)\n",
    "        return inputs\n",
    "\n",
    "        # if self.transforms:\n",
    "        #     sample = self.transforms(sample)\n",
    "\n",
    "        # return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "798bdc15-2101-4eb7-a52f-6748fd70d5d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-14T11:28:26.756919Z",
     "iopub.status.busy": "2024-10-14T11:28:26.756780Z",
     "iopub.status.idle": "2024-10-14T11:28:27.051779Z",
     "shell.execute_reply": "2024-10-14T11:28:27.051275Z",
     "shell.execute_reply.started": "2024-10-14T11:28:26.756906Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_fashion_data = CustomFashionManager(csv_file=train_df,\n",
    "                                    root_dir='train_images')\n",
    "val_fashion_data = CustomFashionManager(csv_file=val_df,\n",
    "                                    root_dir='train_images')\n",
    "test_fashion_data = CustomFashionManager(csv_file=test_df,root_dir='train_images')\n",
    "\n",
    "fig = plt.figure()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "06d3d845-6704-4cc0-acdd-a12ee3f1f8c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-14T11:28:27.578464Z",
     "iopub.status.busy": "2024-10-14T11:28:27.578296Z",
     "iopub.status.idle": "2024-10-14T11:28:28.848764Z",
     "shell.execute_reply": "2024-10-14T11:28:28.848059Z",
     "shell.execute_reply.started": "2024-10-14T11:28:27.578419Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from typing import List\n",
    "from transformers import ViTConfig,ViTPreTrainedModel\n",
    "\n",
    "\n",
    "class CustomConfig(ViTConfig):\n",
    "    def __init__(self,num_classes_per_label:List[int]=[1],**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_classes_per_label = num_classes_per_label\n",
    "\n",
    "class MultiLabelMultiClassViT(ViTPreTrainedModel):\n",
    "    config_class=CustomConfig\n",
    "    def __init__(self, config: CustomConfig) -> None:\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.vit = ViTModel(config, add_pooling_layer=False)\n",
    "        self.classifiers = nn.ModuleList([\n",
    "            nn.Linear(config.hidden_size, num_classes) \n",
    "            for num_classes in config.num_classes_per_label\n",
    "        ])\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "\n",
    "    def forward(self, pixel_values,labels=None):\n",
    "        outputs = self.vit(pixel_values).last_hidden_state[:, 0, :]  # CLS token representation\n",
    "        logits = [classifier(outputs) for classifier in self.classifiers]\n",
    "        if labels is not None:\n",
    "            loss=0\n",
    "            for i in range(len(logits)):\n",
    "                target=labels[:,i]\n",
    "                loss += torch.nn.functional.cross_entropy(logits[i], target)\n",
    "            return {\"loss\": loss, \"logits\": logits}\n",
    "        return {\"logits\": logits}\n",
    "\n",
    "# Example usage\n",
    "num_labels = len(trackNum)  # For example, 5 different labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9be5327a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 3, 3, 4, 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MultiLabelMultiClassViT were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['classifiers.0.bias', 'classifiers.0.weight', 'classifiers.1.bias', 'classifiers.1.weight', 'classifiers.2.bias', 'classifiers.2.weight', 'classifiers.3.bias', 'classifiers.3.weight', 'classifiers.4.bias', 'classifiers.4.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "from sklearn.metrics import classification_report\n",
    "batch_size = 32\n",
    "def collate_fn(batch):\n",
    "    return {\n",
    "        'pixel_values': torch.cat([x['pixel_values'] for x in batch],dim=0),\n",
    "        'labels': torch.stack([x['labels'] for x in batch])\n",
    "    }\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    logits = pred.predictions\n",
    "    labels=pred.label_ids\n",
    "    probs = np.stack([np.argmax(logit,axis=1) for logit in logits])\n",
    "    probs=probs.T\n",
    "    report=classification_report(labels.flatten(),probs.flatten(),output_dict=True)\n",
    "    return {'accuracy': report['accuracy'],\"macro avg f1\":report['macro avg']['f1-score']}\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "  output_dir=\"./vit\",\n",
    "  per_device_train_batch_size=48,\n",
    "  per_device_eval_batch_size=48,\n",
    "  evaluation_strategy=\"epoch\",\n",
    "  save_strategy=\"epoch\",\n",
    "  num_train_epochs=1,\n",
    "  fp16=True,\n",
    "  learning_rate=2e-4,\n",
    "  save_total_limit=1,\n",
    "  remove_unused_columns=False,\n",
    "  push_to_hub=False,\n",
    "  report_to='wandb',\n",
    "  load_best_model_at_end=True,\n",
    ")\n",
    "config=ViTConfig.from_pretrained(model_name)\n",
    "config=CustomConfig(num_classes_per_label=trackNum,**config.to_dict())\n",
    "model = MultiLabelMultiClassViT.from_pretrained(model_name,config=config)\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=train_fashion_data,\n",
    "    eval_dataset=val_fashion_data,\n",
    "    data_collator=collate_fn,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=processor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "dccb554e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='53' max='53' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [53/53 01:01, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro avg f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.950229</td>\n",
       "      <td>0.761396</td>\n",
       "      <td>0.600754</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=53, training_loss=3.3023929236070164, metrics={'train_runtime': 63.6514, 'train_samples_per_second': 79.904, 'train_steps_per_second': 0.833, 'total_flos': 3.941807777569751e+17, 'train_loss': 3.3023929236070164, 'epoch': 1.0})"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c3474278",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8' max='8' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8/8 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 2.8807997703552246,\n",
       " 'eval_accuracy': 0.7627777777777778,\n",
       " 'eval_macro avg f1': 0.6150085912468228,\n",
       " 'eval_runtime': 4.8588,\n",
       " 'eval_samples_per_second': 148.186,\n",
       " 'eval_steps_per_second': 1.647,\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(test_fashion_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "6e087c6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "del model, trainer\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mhcp4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
