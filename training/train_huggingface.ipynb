{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "08046f22-db0f-45bb-8aa5-18309a23a1e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-14T11:28:23.940808Z",
     "iopub.status.busy": "2024-10-14T11:28:23.940248Z",
     "iopub.status.idle": "2024-10-14T11:28:26.614016Z",
     "shell.execute_reply": "2024-10-14T11:28:26.613452Z",
     "shell.execute_reply.started": "2024-10-14T11:28:23.940788Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "import torch\n",
    "import pandas as pd\n",
    "from PIL import ImageDraw, ImageFont, Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import ViTModel\n",
    "from torchinfo import summary  # \n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3438730c-888d-4fd7-b57e-8ff63ab4f94a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-14T11:28:26.616389Z",
     "iopub.status.busy": "2024-10-14T11:28:26.616065Z",
     "iopub.status.idle": "2024-10-14T11:28:26.620522Z",
     "shell.execute_reply": "2024-10-14T11:28:26.619968Z",
     "shell.execute_reply.started": "2024-10-14T11:28:26.616371Z"
    }
   },
   "outputs": [],
   "source": [
    "DEVICE=\"cuda:1\"\n",
    "def setAllSeeds(seed):\n",
    "  os.environ['MY_GLOBAL_SEED'] = str(seed)\n",
    "  random.seed(seed)\n",
    "  np.random.seed(seed)\n",
    "  torch.manual_seed(seed)\n",
    "  torch.cuda.manual_seed_all(seed)\n",
    "setAllSeeds(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "78a86c36-a933-42bf-92c1-bd6cfbf0975b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-14T11:28:26.625620Z",
     "iopub.status.busy": "2024-10-14T11:28:26.625171Z",
     "iopub.status.idle": "2024-10-14T11:28:26.717777Z",
     "shell.execute_reply": "2024-10-14T11:28:26.717238Z",
     "shell.execute_reply.started": "2024-10-14T11:28:26.625599Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Men Tshirts' 'Sarees' 'Kurtis' 'Women Tshirts' 'Women Tops & Tunics']\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"train.csv\")\n",
    "categories=df[\"Category\"].unique()\n",
    "print(categories)\n",
    "category=categories[1]\n",
    "df = df[df[\"Category\"]==category]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "218fb75d-1b90-416b-9a41-288de39def1b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-14T11:28:26.719372Z",
     "iopub.status.busy": "2024-10-14T11:28:26.719005Z",
     "iopub.status.idle": "2024-10-14T11:28:26.725099Z",
     "shell.execute_reply": "2024-10-14T11:28:26.724553Z",
     "shell.execute_reply.started": "2024-10-14T11:28:26.719356Z"
    }
   },
   "outputs": [],
   "source": [
    "delCol = []\n",
    "idxCol = []\n",
    "trackNum = []\n",
    "for i in range(1,11):\n",
    "    uniName = df[\"attr_\"+str(i)].unique()\n",
    "    # print(len(uniName))\n",
    "    if(len(uniName)==1):\n",
    "        delCol.append(\"attr_\"+str(i))\n",
    "    else:\n",
    "        idxCol.append(\"attr_\"+str(i))\n",
    "        trackNum.append(len(uniName))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "52d305da-99e2-424a-b774-b1ffdbd56fcf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-14T11:28:26.732549Z",
     "iopub.status.busy": "2024-10-14T11:28:26.732338Z",
     "iopub.status.idle": "2024-10-14T11:28:26.736656Z",
     "shell.execute_reply": "2024-10-14T11:28:26.736114Z",
     "shell.execute_reply.started": "2024-10-14T11:28:26.732533Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18346, 13)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop(delCol,axis=1)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "02ce1ccc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id              0\n",
       "Category        0\n",
       "len             0\n",
       "attr_1      10461\n",
       "attr_2        667\n",
       "attr_3       2485\n",
       "attr_4        450\n",
       "attr_5        697\n",
       "attr_6      13336\n",
       "attr_7       9450\n",
       "attr_8       1881\n",
       "attr_9       4043\n",
       "attr_10       528\n",
       "dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "df2da111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: {0: 'same as saree', 1: 'solid', 2: 'same as border', 3: 'default'}, 1: {0: 'woven design', 1: 'zari', 2: 'no border', 3: 'solid', 4: 'default', 5: 'temple border'}, 2: {0: 'small border', 1: 'big border', 2: 'no border'}, 3: {0: 'multicolor', 1: 'cream', 2: 'white', 3: 'default', 4: 'navy blue', 5: 'yellow', 6: 'green', 7: 'pink'}, 4: {0: 'party', 1: 'traditional', 2: 'daily', 3: 'wedding'}, 5: {0: 'jacquard', 1: 'default', 2: 'tassels and latkans'}, 6: {0: 'woven design', 1: 'same as saree', 2: 'default', 3: 'zari woven'}, 7: {0: 'zari woven', 1: 'woven design', 2: 'default', 3: 'solid', 4: 'printed'}, 8: {0: 'applique', 1: 'elephant', 2: 'floral', 3: 'ethnic motif', 4: 'peacock', 5: 'default', 6: 'solid', 7: 'checked', 8: 'botanical'}, 9: {0: 'no', 1: 'yes'}}\n",
      "{0: {'same as saree': 0, 'solid': 1, 'same as border': 2, 'default': 3}, 1: {'woven design': 0, 'zari': 1, 'no border': 2, 'solid': 3, 'default': 4, 'temple border': 5}, 2: {'small border': 0, 'big border': 1, 'no border': 2}, 3: {'multicolor': 0, 'cream': 1, 'white': 2, 'default': 3, 'navy blue': 4, 'yellow': 5, 'green': 6, 'pink': 7}, 4: {'party': 0, 'traditional': 1, 'daily': 2, 'wedding': 3}, 5: {'jacquard': 0, 'default': 1, 'tassels and latkans': 2}, 6: {'woven design': 0, 'same as saree': 1, 'default': 2, 'zari woven': 3}, 7: {'zari woven': 0, 'woven design': 1, 'default': 2, 'solid': 3, 'printed': 4}, 8: {'applique': 0, 'elephant': 1, 'floral': 2, 'ethnic motif': 3, 'peacock': 4, 'default': 5, 'solid': 6, 'checked': 7, 'botanical': 8}, 9: {'no': 0, 'yes': 1}}\n",
      "{0: 'attr_1', 1: 'attr_2', 2: 'attr_3', 3: 'attr_4', 4: 'attr_5', 5: 'attr_6', 6: 'attr_7', 7: 'attr_8', 8: 'attr_9', 9: 'attr_10'}\n"
     ]
    }
   ],
   "source": [
    "id2label={}\n",
    "label2id={}\n",
    "attrs={}\n",
    "total_attr=len(df.columns)\n",
    "for i in range(3,total_attr):\n",
    "    labels=df[df.columns[i]].dropna().unique()\n",
    "    # print(df.columns[i],labels)\n",
    "    id2label[i-3]={k:labels[k] for k in range(len(labels))}\n",
    "    label2id[i-3]={labels[k]:k for k in range(len(labels))}\n",
    "    attrs[i-3]=df.columns[i]\n",
    "print(id2label)\n",
    "print(label2id)\n",
    "print(attrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f394a515",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Category</th>\n",
       "      <th>len</th>\n",
       "      <th>attr_1</th>\n",
       "      <th>attr_2</th>\n",
       "      <th>attr_3</th>\n",
       "      <th>attr_4</th>\n",
       "      <th>attr_5</th>\n",
       "      <th>attr_6</th>\n",
       "      <th>attr_7</th>\n",
       "      <th>attr_8</th>\n",
       "      <th>attr_9</th>\n",
       "      <th>attr_10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7267</th>\n",
       "      <td>7432</td>\n",
       "      <td>Sarees</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7268</th>\n",
       "      <td>7433</td>\n",
       "      <td>Sarees</td>\n",
       "      <td>10</td>\n",
       "      <td>-100</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7269</th>\n",
       "      <td>7434</td>\n",
       "      <td>Sarees</td>\n",
       "      <td>10</td>\n",
       "      <td>-100</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7270</th>\n",
       "      <td>7435</td>\n",
       "      <td>Sarees</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>-100</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7271</th>\n",
       "      <td>7436</td>\n",
       "      <td>Sarees</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>2</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id Category  len  attr_1  attr_2  attr_3  attr_4  attr_5  attr_6  \\\n",
       "7267  7432   Sarees   10       0       0       0       0       0       0   \n",
       "7268  7433   Sarees   10    -100       1       0       1       1    -100   \n",
       "7269  7434   Sarees   10    -100       1       0       2       0    -100   \n",
       "7270  7435   Sarees   10       0       0       1       3       1    -100   \n",
       "7271  7436   Sarees   10       1       2    -100    -100       2    -100   \n",
       "\n",
       "      attr_7  attr_8  attr_9  attr_10  \n",
       "7267       0       0       0        0  \n",
       "7268    -100       0       1        0  \n",
       "7269    -100       0       2        0  \n",
       "7270       1       0       3        0  \n",
       "7271    -100    -100    -100        1  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def categorize(example):\n",
    "    for i in attrs:\n",
    "        # print(example[attrs[i]],type(example[attrs[i]]),pd.isna(example[attrs[i]]))\n",
    "        if not pd.isna(example[attrs[i]]):\n",
    "            example[attrs[i]]=label2id[i][example[attrs[i]]]\n",
    "        else:\n",
    "            example[attrs[i]]=-100\n",
    "    return example\n",
    "df=df.apply(categorize,axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f1df7b6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id          0\n",
       "Category    0\n",
       "len         0\n",
       "attr_1      0\n",
       "attr_2      0\n",
       "attr_3      0\n",
       "attr_4      0\n",
       "attr_5      0\n",
       "attr_6      0\n",
       "attr_7      0\n",
       "attr_8      0\n",
       "attr_9      0\n",
       "attr_10     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "92d71000",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor\n",
    "# model_name = 'google/vit-base-patch16-224'\n",
    "model_name=\"facebook/deit-base-distilled-patch16-224\"\n",
    "processor = AutoImageProcessor.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "332990dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, val_df = train_test_split(df, test_size=0.3)\n",
    "val_df,test_df=train_test_split(val_df,test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1113befc-d9c0-427e-98c8-467977f0f7ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-14T11:28:26.751798Z",
     "iopub.status.busy": "2024-10-14T11:28:26.751660Z",
     "iopub.status.idle": "2024-10-14T11:28:26.756144Z",
     "shell.execute_reply": "2024-10-14T11:28:26.755669Z",
     "shell.execute_reply.started": "2024-10-14T11:28:26.751778Z"
    }
   },
   "outputs": [],
   "source": [
    "class CustomFashionManager(Dataset):\n",
    "    def __init__(self,csv_file, root_dir=\"./\",transforms =None):\n",
    "        self.fashionItems = csv_file\n",
    "        self.root_dir = root_dir\n",
    "        self.transforms = transforms\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.fashionItems)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_name = os.path.join(self.root_dir,f\"{self.fashionItems.iloc[idx, 0]:06d}\"+'.jpg')\n",
    "        image = Image.open(img_name)\n",
    "        attributes = self.fashionItems.iloc[idx, 3:]\n",
    "        attributes = np.array(attributes)\n",
    "        attributes = attributes.astype('float')\n",
    "        # print(attributes.shape)\n",
    "        # attributes = attributes.astype('float').reshape(-1, len(attributes))\n",
    "        # swap color axis because\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C X H X W\n",
    "        inputs=processor(image, return_tensors='pt')\n",
    "        inputs['labels']=torch.tensor(attributes, dtype=torch.long)\n",
    "        return inputs\n",
    "\n",
    "        # if self.transforms:\n",
    "        #     sample = self.transforms(sample)\n",
    "\n",
    "        # return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "798bdc15-2101-4eb7-a52f-6748fd70d5d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-14T11:28:26.756919Z",
     "iopub.status.busy": "2024-10-14T11:28:26.756780Z",
     "iopub.status.idle": "2024-10-14T11:28:27.051779Z",
     "shell.execute_reply": "2024-10-14T11:28:27.051275Z",
     "shell.execute_reply.started": "2024-10-14T11:28:26.756906Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_fashion_data = CustomFashionManager(csv_file=train_df,\n",
    "                                    root_dir='train_images')\n",
    "val_fashion_data = CustomFashionManager(csv_file=val_df,\n",
    "                                    root_dir='train_images')\n",
    "test_fashion_data = CustomFashionManager(csv_file=test_df,root_dir='train_images')\n",
    "\n",
    "fig = plt.figure()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "06d3d845-6704-4cc0-acdd-a12ee3f1f8c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-14T11:28:27.578464Z",
     "iopub.status.busy": "2024-10-14T11:28:27.578296Z",
     "iopub.status.idle": "2024-10-14T11:28:28.848764Z",
     "shell.execute_reply": "2024-10-14T11:28:28.848059Z",
     "shell.execute_reply.started": "2024-10-14T11:28:27.578419Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from typing import List\n",
    "from transformers import ViTConfig,ViTPreTrainedModel,DeiTConfig,DeiTPreTrainedModel,DeiTModel\n",
    "\n",
    "\n",
    "class CustomConfig(DeiTConfig):\n",
    "    def __init__(self,num_classes_per_label:List[int]=[1],**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_classes_per_label = num_classes_per_label\n",
    "\n",
    "class MultiLabelMultiClassViT(DeiTPreTrainedModel):\n",
    "    config_class=CustomConfig\n",
    "    def __init__(self, config: CustomConfig) -> None:\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.vit = DeiTModel(config, add_pooling_layer=False)\n",
    "        # for param in self.vit.parameters():\n",
    "        #     param.requires_grad = False\n",
    "        self.classifiers = nn.ModuleList([\n",
    "            nn.Linear(config.hidden_size, num_classes) \n",
    "            for num_classes in config.num_classes_per_label\n",
    "        ])\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "\n",
    "    def forward(self, pixel_values,labels=None):\n",
    "        outputs = self.vit(pixel_values).last_hidden_state[:, 0, :]  # CLS token representation\n",
    "        logits = [classifier(outputs) for classifier in self.classifiers]\n",
    "        if labels is not None:\n",
    "            loss=0\n",
    "            for i in range(len(logits)):\n",
    "                target=labels[:,i]\n",
    "                loss += torch.nn.functional.cross_entropy(logits[i], target)\n",
    "            return {\"loss\": loss, \"logits\": logits}\n",
    "        return {\"logits\": logits}\n",
    "\n",
    "# Example usage\n",
    "num_labels = len(trackNum)  # For example, 5 different labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9be5327a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type deit to instantiate a model of type vit. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of MultiLabelMultiClassViT were not initialized from the model checkpoint at facebook/deit-base-distilled-patch16-224 and are newly initialized: ['deit.classifiers.0.bias', 'deit.classifiers.0.weight', 'deit.classifiers.1.bias', 'deit.classifiers.1.weight', 'deit.classifiers.2.bias', 'deit.classifiers.2.weight', 'deit.classifiers.3.bias', 'deit.classifiers.3.weight', 'deit.classifiers.4.bias', 'deit.classifiers.4.weight', 'deit.classifiers.5.bias', 'deit.classifiers.5.weight', 'deit.classifiers.6.bias', 'deit.classifiers.6.weight', 'deit.classifiers.7.bias', 'deit.classifiers.7.weight', 'deit.classifiers.8.bias', 'deit.classifiers.8.weight', 'deit.classifiers.9.bias', 'deit.classifiers.9.weight', 'deit.vit.embeddings.cls_token', 'deit.vit.embeddings.distillation_token', 'deit.vit.embeddings.patch_embeddings.projection.bias', 'deit.vit.embeddings.patch_embeddings.projection.weight', 'deit.vit.embeddings.position_embeddings', 'deit.vit.encoder.layer.0.attention.attention.key.bias', 'deit.vit.encoder.layer.0.attention.attention.key.weight', 'deit.vit.encoder.layer.0.attention.attention.query.bias', 'deit.vit.encoder.layer.0.attention.attention.query.weight', 'deit.vit.encoder.layer.0.attention.attention.value.bias', 'deit.vit.encoder.layer.0.attention.attention.value.weight', 'deit.vit.encoder.layer.0.attention.output.dense.bias', 'deit.vit.encoder.layer.0.attention.output.dense.weight', 'deit.vit.encoder.layer.0.intermediate.dense.bias', 'deit.vit.encoder.layer.0.intermediate.dense.weight', 'deit.vit.encoder.layer.0.layernorm_after.bias', 'deit.vit.encoder.layer.0.layernorm_after.weight', 'deit.vit.encoder.layer.0.layernorm_before.bias', 'deit.vit.encoder.layer.0.layernorm_before.weight', 'deit.vit.encoder.layer.0.output.dense.bias', 'deit.vit.encoder.layer.0.output.dense.weight', 'deit.vit.encoder.layer.1.attention.attention.key.bias', 'deit.vit.encoder.layer.1.attention.attention.key.weight', 'deit.vit.encoder.layer.1.attention.attention.query.bias', 'deit.vit.encoder.layer.1.attention.attention.query.weight', 'deit.vit.encoder.layer.1.attention.attention.value.bias', 'deit.vit.encoder.layer.1.attention.attention.value.weight', 'deit.vit.encoder.layer.1.attention.output.dense.bias', 'deit.vit.encoder.layer.1.attention.output.dense.weight', 'deit.vit.encoder.layer.1.intermediate.dense.bias', 'deit.vit.encoder.layer.1.intermediate.dense.weight', 'deit.vit.encoder.layer.1.layernorm_after.bias', 'deit.vit.encoder.layer.1.layernorm_after.weight', 'deit.vit.encoder.layer.1.layernorm_before.bias', 'deit.vit.encoder.layer.1.layernorm_before.weight', 'deit.vit.encoder.layer.1.output.dense.bias', 'deit.vit.encoder.layer.1.output.dense.weight', 'deit.vit.encoder.layer.10.attention.attention.key.bias', 'deit.vit.encoder.layer.10.attention.attention.key.weight', 'deit.vit.encoder.layer.10.attention.attention.query.bias', 'deit.vit.encoder.layer.10.attention.attention.query.weight', 'deit.vit.encoder.layer.10.attention.attention.value.bias', 'deit.vit.encoder.layer.10.attention.attention.value.weight', 'deit.vit.encoder.layer.10.attention.output.dense.bias', 'deit.vit.encoder.layer.10.attention.output.dense.weight', 'deit.vit.encoder.layer.10.intermediate.dense.bias', 'deit.vit.encoder.layer.10.intermediate.dense.weight', 'deit.vit.encoder.layer.10.layernorm_after.bias', 'deit.vit.encoder.layer.10.layernorm_after.weight', 'deit.vit.encoder.layer.10.layernorm_before.bias', 'deit.vit.encoder.layer.10.layernorm_before.weight', 'deit.vit.encoder.layer.10.output.dense.bias', 'deit.vit.encoder.layer.10.output.dense.weight', 'deit.vit.encoder.layer.11.attention.attention.key.bias', 'deit.vit.encoder.layer.11.attention.attention.key.weight', 'deit.vit.encoder.layer.11.attention.attention.query.bias', 'deit.vit.encoder.layer.11.attention.attention.query.weight', 'deit.vit.encoder.layer.11.attention.attention.value.bias', 'deit.vit.encoder.layer.11.attention.attention.value.weight', 'deit.vit.encoder.layer.11.attention.output.dense.bias', 'deit.vit.encoder.layer.11.attention.output.dense.weight', 'deit.vit.encoder.layer.11.intermediate.dense.bias', 'deit.vit.encoder.layer.11.intermediate.dense.weight', 'deit.vit.encoder.layer.11.layernorm_after.bias', 'deit.vit.encoder.layer.11.layernorm_after.weight', 'deit.vit.encoder.layer.11.layernorm_before.bias', 'deit.vit.encoder.layer.11.layernorm_before.weight', 'deit.vit.encoder.layer.11.output.dense.bias', 'deit.vit.encoder.layer.11.output.dense.weight', 'deit.vit.encoder.layer.2.attention.attention.key.bias', 'deit.vit.encoder.layer.2.attention.attention.key.weight', 'deit.vit.encoder.layer.2.attention.attention.query.bias', 'deit.vit.encoder.layer.2.attention.attention.query.weight', 'deit.vit.encoder.layer.2.attention.attention.value.bias', 'deit.vit.encoder.layer.2.attention.attention.value.weight', 'deit.vit.encoder.layer.2.attention.output.dense.bias', 'deit.vit.encoder.layer.2.attention.output.dense.weight', 'deit.vit.encoder.layer.2.intermediate.dense.bias', 'deit.vit.encoder.layer.2.intermediate.dense.weight', 'deit.vit.encoder.layer.2.layernorm_after.bias', 'deit.vit.encoder.layer.2.layernorm_after.weight', 'deit.vit.encoder.layer.2.layernorm_before.bias', 'deit.vit.encoder.layer.2.layernorm_before.weight', 'deit.vit.encoder.layer.2.output.dense.bias', 'deit.vit.encoder.layer.2.output.dense.weight', 'deit.vit.encoder.layer.3.attention.attention.key.bias', 'deit.vit.encoder.layer.3.attention.attention.key.weight', 'deit.vit.encoder.layer.3.attention.attention.query.bias', 'deit.vit.encoder.layer.3.attention.attention.query.weight', 'deit.vit.encoder.layer.3.attention.attention.value.bias', 'deit.vit.encoder.layer.3.attention.attention.value.weight', 'deit.vit.encoder.layer.3.attention.output.dense.bias', 'deit.vit.encoder.layer.3.attention.output.dense.weight', 'deit.vit.encoder.layer.3.intermediate.dense.bias', 'deit.vit.encoder.layer.3.intermediate.dense.weight', 'deit.vit.encoder.layer.3.layernorm_after.bias', 'deit.vit.encoder.layer.3.layernorm_after.weight', 'deit.vit.encoder.layer.3.layernorm_before.bias', 'deit.vit.encoder.layer.3.layernorm_before.weight', 'deit.vit.encoder.layer.3.output.dense.bias', 'deit.vit.encoder.layer.3.output.dense.weight', 'deit.vit.encoder.layer.4.attention.attention.key.bias', 'deit.vit.encoder.layer.4.attention.attention.key.weight', 'deit.vit.encoder.layer.4.attention.attention.query.bias', 'deit.vit.encoder.layer.4.attention.attention.query.weight', 'deit.vit.encoder.layer.4.attention.attention.value.bias', 'deit.vit.encoder.layer.4.attention.attention.value.weight', 'deit.vit.encoder.layer.4.attention.output.dense.bias', 'deit.vit.encoder.layer.4.attention.output.dense.weight', 'deit.vit.encoder.layer.4.intermediate.dense.bias', 'deit.vit.encoder.layer.4.intermediate.dense.weight', 'deit.vit.encoder.layer.4.layernorm_after.bias', 'deit.vit.encoder.layer.4.layernorm_after.weight', 'deit.vit.encoder.layer.4.layernorm_before.bias', 'deit.vit.encoder.layer.4.layernorm_before.weight', 'deit.vit.encoder.layer.4.output.dense.bias', 'deit.vit.encoder.layer.4.output.dense.weight', 'deit.vit.encoder.layer.5.attention.attention.key.bias', 'deit.vit.encoder.layer.5.attention.attention.key.weight', 'deit.vit.encoder.layer.5.attention.attention.query.bias', 'deit.vit.encoder.layer.5.attention.attention.query.weight', 'deit.vit.encoder.layer.5.attention.attention.value.bias', 'deit.vit.encoder.layer.5.attention.attention.value.weight', 'deit.vit.encoder.layer.5.attention.output.dense.bias', 'deit.vit.encoder.layer.5.attention.output.dense.weight', 'deit.vit.encoder.layer.5.intermediate.dense.bias', 'deit.vit.encoder.layer.5.intermediate.dense.weight', 'deit.vit.encoder.layer.5.layernorm_after.bias', 'deit.vit.encoder.layer.5.layernorm_after.weight', 'deit.vit.encoder.layer.5.layernorm_before.bias', 'deit.vit.encoder.layer.5.layernorm_before.weight', 'deit.vit.encoder.layer.5.output.dense.bias', 'deit.vit.encoder.layer.5.output.dense.weight', 'deit.vit.encoder.layer.6.attention.attention.key.bias', 'deit.vit.encoder.layer.6.attention.attention.key.weight', 'deit.vit.encoder.layer.6.attention.attention.query.bias', 'deit.vit.encoder.layer.6.attention.attention.query.weight', 'deit.vit.encoder.layer.6.attention.attention.value.bias', 'deit.vit.encoder.layer.6.attention.attention.value.weight', 'deit.vit.encoder.layer.6.attention.output.dense.bias', 'deit.vit.encoder.layer.6.attention.output.dense.weight', 'deit.vit.encoder.layer.6.intermediate.dense.bias', 'deit.vit.encoder.layer.6.intermediate.dense.weight', 'deit.vit.encoder.layer.6.layernorm_after.bias', 'deit.vit.encoder.layer.6.layernorm_after.weight', 'deit.vit.encoder.layer.6.layernorm_before.bias', 'deit.vit.encoder.layer.6.layernorm_before.weight', 'deit.vit.encoder.layer.6.output.dense.bias', 'deit.vit.encoder.layer.6.output.dense.weight', 'deit.vit.encoder.layer.7.attention.attention.key.bias', 'deit.vit.encoder.layer.7.attention.attention.key.weight', 'deit.vit.encoder.layer.7.attention.attention.query.bias', 'deit.vit.encoder.layer.7.attention.attention.query.weight', 'deit.vit.encoder.layer.7.attention.attention.value.bias', 'deit.vit.encoder.layer.7.attention.attention.value.weight', 'deit.vit.encoder.layer.7.attention.output.dense.bias', 'deit.vit.encoder.layer.7.attention.output.dense.weight', 'deit.vit.encoder.layer.7.intermediate.dense.bias', 'deit.vit.encoder.layer.7.intermediate.dense.weight', 'deit.vit.encoder.layer.7.layernorm_after.bias', 'deit.vit.encoder.layer.7.layernorm_after.weight', 'deit.vit.encoder.layer.7.layernorm_before.bias', 'deit.vit.encoder.layer.7.layernorm_before.weight', 'deit.vit.encoder.layer.7.output.dense.bias', 'deit.vit.encoder.layer.7.output.dense.weight', 'deit.vit.encoder.layer.8.attention.attention.key.bias', 'deit.vit.encoder.layer.8.attention.attention.key.weight', 'deit.vit.encoder.layer.8.attention.attention.query.bias', 'deit.vit.encoder.layer.8.attention.attention.query.weight', 'deit.vit.encoder.layer.8.attention.attention.value.bias', 'deit.vit.encoder.layer.8.attention.attention.value.weight', 'deit.vit.encoder.layer.8.attention.output.dense.bias', 'deit.vit.encoder.layer.8.attention.output.dense.weight', 'deit.vit.encoder.layer.8.intermediate.dense.bias', 'deit.vit.encoder.layer.8.intermediate.dense.weight', 'deit.vit.encoder.layer.8.layernorm_after.bias', 'deit.vit.encoder.layer.8.layernorm_after.weight', 'deit.vit.encoder.layer.8.layernorm_before.bias', 'deit.vit.encoder.layer.8.layernorm_before.weight', 'deit.vit.encoder.layer.8.output.dense.bias', 'deit.vit.encoder.layer.8.output.dense.weight', 'deit.vit.encoder.layer.9.attention.attention.key.bias', 'deit.vit.encoder.layer.9.attention.attention.key.weight', 'deit.vit.encoder.layer.9.attention.attention.query.bias', 'deit.vit.encoder.layer.9.attention.attention.query.weight', 'deit.vit.encoder.layer.9.attention.attention.value.bias', 'deit.vit.encoder.layer.9.attention.attention.value.weight', 'deit.vit.encoder.layer.9.attention.output.dense.bias', 'deit.vit.encoder.layer.9.attention.output.dense.weight', 'deit.vit.encoder.layer.9.intermediate.dense.bias', 'deit.vit.encoder.layer.9.intermediate.dense.weight', 'deit.vit.encoder.layer.9.layernorm_after.bias', 'deit.vit.encoder.layer.9.layernorm_after.weight', 'deit.vit.encoder.layer.9.layernorm_before.bias', 'deit.vit.encoder.layer.9.layernorm_before.weight', 'deit.vit.encoder.layer.9.output.dense.bias', 'deit.vit.encoder.layer.9.output.dense.weight', 'deit.vit.layernorm.bias', 'deit.vit.layernorm.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "from sklearn.metrics import classification_report\n",
    "batch_size = 32\n",
    "def collate_fn(batch):\n",
    "    return {\n",
    "        'pixel_values': torch.cat([x['pixel_values'] for x in batch],dim=0),\n",
    "        'labels': torch.stack([x['labels'] for x in batch])\n",
    "    }\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    logits = pred.predictions\n",
    "    labels=pred.label_ids\n",
    "    probs = np.stack([np.argmax(logit,axis=1) for logit in logits])\n",
    "    probs=probs.T\n",
    "    # truth_labels=[]\n",
    "    # preds=[]\n",
    "    # for i in range(len(probs)):\n",
    "    #     pred=[]\n",
    "    #     true=[]\n",
    "    #     for j in range(len(probs[i])):\n",
    "    #         pred.append(id2label[j][probs[i][j]])\n",
    "    #         if labels[i][j]==-100:\n",
    "    #             true.append(-100)\n",
    "    #         else:\n",
    "    #             true.append(id2label[j][labels[i][j]])\n",
    "    #     preds.append(pred)\n",
    "    #     truth_labels.append(true)\n",
    "\n",
    "    # preds=np.array(preds)\n",
    "    # truth_labels=np.array(truth_labels)\n",
    "\n",
    "    # labels=truth_labels.flatten()\n",
    "    # probs=preds.flatten()\n",
    "\n",
    "    labels=labels.flatten()\n",
    "    probs=probs.flatten()\n",
    "    non_padding_indices = [i for i, label in enumerate(labels) if label != '-100']\n",
    "\n",
    "# Use the filtered indices to get non-padding true and predicted labels\n",
    "    labels = [labels[i] for i in non_padding_indices]\n",
    "    probs = [probs[i] for i in non_padding_indices]\n",
    "\n",
    "    # print(classification_report(labels,probs))\n",
    "    report=classification_report(labels,probs,output_dict=True)\n",
    "    return {'accuracy': report['accuracy'],\"macro avg f1\":report['macro avg']['f1-score']}\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "  output_dir=\"./vit3/\"+category,\n",
    "  per_device_train_batch_size=64,\n",
    "  per_device_eval_batch_size=64,\n",
    "  evaluation_strategy=\"epoch\",\n",
    "  save_strategy=\"epoch\",\n",
    "  logging_strategy=\"epoch\",\n",
    "  num_train_epochs=5,\n",
    "  fp16=True,\n",
    "  learning_rate=3e-4,\n",
    "  save_total_limit=1,\n",
    "  remove_unused_columns=False,\n",
    "  push_to_hub=False,\n",
    "  report_to='wandb',\n",
    "  load_best_model_at_end=True,\n",
    "  metric_for_best_model=\"macro avg f1\"\n",
    ")\n",
    "config=ViTConfig.from_pretrained(model_name)\n",
    "config=CustomConfig(num_classes_per_label=trackNum,**config.to_dict())\n",
    "model = MultiLabelMultiClassViT.from_pretrained(model_name,config=config)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=train_fashion_data,\n",
    "    eval_dataset=val_fashion_data,\n",
    "    data_collator=collate_fn,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=processor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccb554e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='559' max='1005' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 559/1005 06:44 < 05:23, 1.38 it/s, Epoch 2.78/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro avg f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>8.429700</td>\n",
       "      <td>7.394188</td>\n",
       "      <td>0.531326</td>\n",
       "      <td>0.354912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>6.952000</td>\n",
       "      <td>6.751800</td>\n",
       "      <td>0.545755</td>\n",
       "      <td>0.409123</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()\n",
    "trainer.save_model(f\"./vit3/{category}/final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c3474278",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='26' max='29' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [26/29 00:13 < 00:01, 1.80 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_fashion_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/trainer.py:3868\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3865\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   3867\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[0;32m-> 3868\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3869\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3870\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvaluation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3871\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[1;32m   3872\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[1;32m   3873\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   3874\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3875\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3876\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3878\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[1;32m   3879\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/trainer.py:4051\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   4048\u001b[0m observed_num_examples \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   4050\u001b[0m \u001b[38;5;66;03m# Main evaluation loop\u001b[39;00m\n\u001b[0;32m-> 4051\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[1;32m   4052\u001b[0m     \u001b[38;5;66;03m# Update the observed num examples\u001b[39;00m\n\u001b[1;32m   4053\u001b[0m     observed_batch_size \u001b[38;5;241m=\u001b[39m find_batch_size(inputs)\n\u001b[1;32m   4054\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m observed_batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/accelerate/data_loader.py:561\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     current_batch \u001b[38;5;241m=\u001b[39m send_to_device(current_batch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice, non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_non_blocking)\n\u001b[1;32m    560\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_state_dict()\n\u001b[0;32m--> 561\u001b[0m next_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskip_batches:\n\u001b[1;32m    563\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m current_batch\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[12], line 24\u001b[0m, in \u001b[0;36mCustomFashionManager.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     18\u001b[0m attributes \u001b[38;5;241m=\u001b[39m attributes\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# print(attributes.shape)\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# attributes = attributes.astype('float').reshape(-1, len(attributes))\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# swap color axis because\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# numpy image: H x W x C\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# torch image: C X H X W\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m inputs\u001b[38;5;241m=\u001b[39m\u001b[43mprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mtensor(attributes, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m inputs\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/image_processing_utils.py:41\u001b[0m, in \u001b[0;36mBaseImageProcessor.__call__\u001b[0;34m(self, images, **kwargs)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, images, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BatchFeature:\n\u001b[1;32m     40\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Preprocess an image or a batch of images.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/utils/generic.py:852\u001b[0m, in \u001b[0;36mfilter_out_non_signature_kwargs.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    843\u001b[0m         cls_prefix \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    845\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    846\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe following named arguments are not valid for `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcls_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    847\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m and were ignored: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minvalid_kwargs_names\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    848\u001b[0m         \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[1;32m    849\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m    850\u001b[0m     )\n\u001b[0;32m--> 852\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mvalid_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/deit/image_processing_deit.py:276\u001b[0m, in \u001b[0;36mDeiTImageProcessor.preprocess\u001b[0;34m(self, images, do_resize, size, resample, do_center_crop, crop_size, do_rescale, rescale_factor, do_normalize, image_mean, image_std, return_tensors, data_format, input_data_format)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images:\n\u001b[1;32m    275\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m do_resize:\n\u001b[0;32m--> 276\u001b[0m         image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_data_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_data_format\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m do_center_crop:\n\u001b[1;32m    279\u001b[0m         image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcenter_crop(image\u001b[38;5;241m=\u001b[39mimage, size\u001b[38;5;241m=\u001b[39mcrop_size, input_data_format\u001b[38;5;241m=\u001b[39minput_data_format)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/deit/image_processing_deit.py:153\u001b[0m, in \u001b[0;36mDeiTImageProcessor.resize\u001b[0;34m(self, image, size, resample, data_format, input_data_format, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `size` dictionary must contain the keys `height` and `width`. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize\u001b[38;5;241m.\u001b[39mkeys()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    152\u001b[0m output_size \u001b[38;5;241m=\u001b[39m (size[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheight\u001b[39m\u001b[38;5;124m\"\u001b[39m], size[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwidth\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m--> 153\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m    \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_data_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_data_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/image_transforms.py:332\u001b[0m, in \u001b[0;36mresize\u001b[0;34m(image, size, resample, reducing_gap, data_format, return_numpy, input_data_format)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(image, PIL\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mImage):\n\u001b[1;32m    331\u001b[0m     do_rescale \u001b[38;5;241m=\u001b[39m _rescale_for_pil_conversion(image)\n\u001b[0;32m--> 332\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mto_pil_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_rescale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_rescale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_data_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_data_format\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    333\u001b[0m height, width \u001b[38;5;241m=\u001b[39m size\n\u001b[1;32m    334\u001b[0m \u001b[38;5;66;03m# PIL images are in the format (width, height)\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/image_transforms.py:210\u001b[0m, in \u001b[0;36mto_pil_image\u001b[0;34m(image, do_rescale, input_data_format)\u001b[0m\n\u001b[1;32m    207\u001b[0m     image \u001b[38;5;241m=\u001b[39m rescale(image, \u001b[38;5;241m255\u001b[39m)\n\u001b[1;32m    209\u001b[0m image \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39muint8)\n\u001b[0;32m--> 210\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPIL\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfromarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/PIL/Image.py:3154\u001b[0m, in \u001b[0;36mfromarray\u001b[0;34m(obj, mode)\u001b[0m\n\u001b[1;32m   3151\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3152\u001b[0m         obj \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mtostring()\n\u001b[0;32m-> 3154\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfrombuffer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mraw\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrawmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/PIL/Image.py:3069\u001b[0m, in \u001b[0;36mfrombuffer\u001b[0;34m(mode, size, data, decoder_name, *args)\u001b[0m\n\u001b[1;32m   3066\u001b[0m         im\u001b[38;5;241m.\u001b[39mreadonly \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   3067\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m im\n\u001b[0;32m-> 3069\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfrombytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/PIL/Image.py:3012\u001b[0m, in \u001b[0;36mfrombytes\u001b[0;34m(mode, size, data, decoder_name, *args)\u001b[0m\n\u001b[1;32m   3009\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m decoder_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m args \u001b[38;5;241m==\u001b[39m ():\n\u001b[1;32m   3010\u001b[0m         args \u001b[38;5;241m=\u001b[39m mode\n\u001b[0;32m-> 3012\u001b[0m     \u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrombytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3013\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m im\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/PIL/Image.py:826\u001b[0m, in \u001b[0;36mImage.frombytes\u001b[0;34m(self, data, decoder_name, *args)\u001b[0m\n\u001b[1;32m    824\u001b[0m d \u001b[38;5;241m=\u001b[39m _getdecoder(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode, decoder_name, args)\n\u001b[1;32m    825\u001b[0m d\u001b[38;5;241m.\u001b[39msetimage(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mim)\n\u001b[0;32m--> 826\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[43md\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m s[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    829\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot enough image data\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.evaluate(test_fashion_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mhcp4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
